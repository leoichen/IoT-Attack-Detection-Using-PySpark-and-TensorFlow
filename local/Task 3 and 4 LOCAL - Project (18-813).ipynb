{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables to change:\n",
    "\n",
    "1. data_train_path/data_test_path: change to path of train and test csv files\n",
    "\n",
    "Also, make sure Postgres JAR file is put in right location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install if necessary \n",
    "# RESTART Kernel after installation\n",
    "\n",
    "#!pip install tensorflow==2.9.2\n",
    "#!pip install numpy==1.21.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # now import the tensorflow module\n",
    "print(tf.__version__)  # make sure the version is 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a new environment for tf which uses python 3.7 instead of python 3.9 which is used in my base env\n",
    "# need to specify env vars so all of pyspark points to python 3.7\n",
    "\n",
    "# So, if there is an error with the python version used, need to replace the paths below with the path\n",
    "# of Python used in the current env (needs to be Python 3.7)\n",
    "\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/anaconda3/envs/tf/bin/python\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"/opt/anaconda3/envs/tf/bin/python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/28 18:47:40 WARN Utils: Your hostname, Leos-MacBook-Pro-3.local resolves to a loopback address: 127.0.0.1; using 10.0.0.76 instead (on interface en0)\n",
      "22/11/28 18:47:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/11/28 18:47:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/11/28 18:47:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APP Name :GenericAppName\n",
      "Master :local[*]\n"
     ]
    }
   ],
   "source": [
    "# set up Spark\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GenericAppName\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#Access SparkContext from your SparkSession\n",
    "print(\"APP Name :\"+ spark.sparkContext.appName);\n",
    "print(\"Master :\"+ spark.sparkContext.master);\n",
    "\n",
    "sqlContext = SQLContext(spark.sparkContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "\n",
    "data_train_path = '../data_folder/train70_reduced.csv'\n",
    "data_test_path = '../data_folder/test30_reduced.csv'\n",
    "\n",
    "df_train = spark.read.csv(data_train_path, header = True, inferSchema = True)\n",
    "df_test = spark.read.csv(data_test_path, header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item Count\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 231646\n",
      "Test: 84351\n",
      "Combined: 315997\n"
     ]
    }
   ],
   "source": [
    "# add column to differentiate b/w train and test sets\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "df_train_cat = df_train.withColumn(\"data_category\", lit(\"train\"))\n",
    "df_test_cat = df_test.withColumn(\"data_category\", lit(\"test\"))\n",
    "\n",
    "print('Item Count\\n')\n",
    "print('Train:', df_train_cat.count())\n",
    "print('Test:', df_test_cat.count())\n",
    "\n",
    "\n",
    "# combine dfs\n",
    "df_combined = df_train_cat.union(df_test_cat)\n",
    "print('Combined:', df_combined.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/28 18:47:54 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write into postgresql db\n",
    "\n",
    "db_properties={}\n",
    "#update your db username\n",
    "db_properties['username']=\"postgres\"\n",
    "#update your db password\n",
    "db_properties['password']=\"bigdata\"\n",
    "#make sure you got the right port number here\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "#make sure you had the Postgres JAR file in the right location\n",
    "db_properties['driver']=\"org.postgresql.Driver\"\n",
    "db_properties['table']= \"mqtt\"\n",
    "\n",
    "# create df with train data \n",
    "df_combined.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", \"bigdata\")\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item Count from PostgreSQL Read: 315997\n"
     ]
    }
   ],
   "source": [
    "# read db to ensure data has been written in correctly\n",
    "df_read = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", \"bigdata\")\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()\n",
    "\n",
    "print('Item Count from PostgreSQL Read:', df_read.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all . with _ in column names to avoid bugs with having . in col name\n",
    "\n",
    "col_names_underscore = ['tcp_flags','tcp_time_delta','tcp_len','mqtt_conack_flags','mqtt_conack_flags_reserved',\n",
    "             'mqtt_conack_flags_sp','mqtt_conack_val','mqtt_conflag_cleansess','mqtt_conflag_passwd',\n",
    "             'mqtt_conflag_qos','mqtt_conflag_reserved','mqtt_conflag_retain','mqtt_conflag_uname',\n",
    "             'mqtt_conflag_willflag','mqtt_conflags','mqtt_dupflag','mqtt_hdrflags','mqtt_kalive',\n",
    "             'mqtt_len','mqtt_msg','mqtt_msgid','mqtt_msgtype','mqtt_proto_len','mqtt_protoname',\n",
    "             'mqtt_qos','mqtt_retain','mqtt_sub_qos','mqtt_suback_qos','mqtt_ver','mqtt_willmsg',\n",
    "             'mqtt_willmsg_len','mqtt_willtopic','mqtt_willtopic_len','target','data_category']\n",
    "\n",
    "df_read_underscore = df_read.toDF(*col_names_underscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all zero columns & absurd cols\n",
    "\n",
    "zero_cols = ['mqtt_conack_flags','mqtt_conack_flags_reserved','mqtt_conack_flags_sp','mqtt_conack_val',\n",
    "            'mqtt_conflag_qos','mqtt_conflag_reserved','mqtt_conflag_retain','mqtt_conflag_willflag',\n",
    "            'mqtt_retain','mqtt_sub_qos','mqtt_suback_qos','mqtt_willmsg',\n",
    "             'mqtt_willmsg_len','mqtt_willtopic','mqtt_willtopic_len']\n",
    "\n",
    "absurd_cols = ['mqtt_msg', 'mqtt_msgid']\n",
    "\n",
    "\n",
    "df_read_underscore_dropped = df_read_underscore.drop(*zero_cols+absurd_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_read_underscore_dropped.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test\n",
    "df_read_train = df_read_underscore_dropped.where(df_read_underscore_dropped['data_category'] == 'train')\n",
    "df_read_test = df_read_underscore_dropped.where(df_read_underscore_dropped['data_category'] == 'test')\n",
    "\n",
    "\n",
    "# drop 'data_category' col\n",
    "df_read_train = df_read_train.drop('data_category')\n",
    "df_read_test = df_read_test.drop('data_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        tcp_time_delta   tcp_len  mqtt_conflag_cleansess  \\\n",
      "tcp_time_delta                1.000000 -0.006952               -0.009565   \n",
      "tcp_len                      -0.006952  1.000000               -0.013370   \n",
      "mqtt_conflag_cleansess       -0.009565 -0.013370                1.000000   \n",
      "mqtt_conflag_passwd          -0.006303 -0.008609                0.658921   \n",
      "mqtt_conflag_uname           -0.006312 -0.008623                0.659891   \n",
      "mqtt_dupflag                 -0.018682  0.159388               -0.024359   \n",
      "mqtt_kalive                  -0.005286 -0.008729                0.552715   \n",
      "mqtt_len                     -0.036825  0.274375                0.001855   \n",
      "mqtt_msgtype                  0.283643  0.085299               -0.056337   \n",
      "mqtt_proto_len               -0.009565 -0.013370                1.000000   \n",
      "mqtt_qos                     -0.037996  0.271391               -0.044270   \n",
      "mqtt_ver                     -0.009565 -0.013370                1.000000   \n",
      "\n",
      "                        mqtt_conflag_passwd  mqtt_conflag_uname  mqtt_dupflag  \\\n",
      "tcp_time_delta                    -0.006303           -0.006312     -0.018682   \n",
      "tcp_len                           -0.008609           -0.008623      0.159388   \n",
      "mqtt_conflag_cleansess             0.658921            0.659891     -0.024359   \n",
      "mqtt_conflag_passwd                1.000000            0.998531     -0.016051   \n",
      "mqtt_conflag_uname                 0.998531            1.000000     -0.016074   \n",
      "mqtt_dupflag                      -0.016051           -0.016074      1.000000   \n",
      "mqtt_kalive                       -0.002539           -0.002543     -0.013464   \n",
      "mqtt_len                          -0.005232           -0.005254      0.544121   \n",
      "mqtt_msgtype                      -0.037122           -0.037177      0.125517   \n",
      "mqtt_proto_len                     0.658921            0.659891     -0.024359   \n",
      "mqtt_qos                          -0.029170           -0.029213      0.550238   \n",
      "mqtt_ver                           0.658921            0.659891     -0.024359   \n",
      "\n",
      "                        mqtt_kalive  mqtt_len  mqtt_msgtype  mqtt_proto_len  \\\n",
      "tcp_time_delta            -0.005286 -0.036825      0.283643       -0.009565   \n",
      "tcp_len                   -0.008729  0.274375      0.085299       -0.013370   \n",
      "mqtt_conflag_cleansess     0.552715  0.001855     -0.056337        1.000000   \n",
      "mqtt_conflag_passwd       -0.002539 -0.005232     -0.037122        0.658921   \n",
      "mqtt_conflag_uname        -0.002543 -0.005254     -0.037177        0.659891   \n",
      "mqtt_dupflag              -0.013464  0.544121      0.125517       -0.024359   \n",
      "mqtt_kalive                1.000000 -0.001461     -0.031139        0.552715   \n",
      "mqtt_len                  -0.001461  1.000000      0.261283        0.001855   \n",
      "mqtt_msgtype              -0.031139  0.261283      1.000000       -0.056337   \n",
      "mqtt_proto_len             0.552715  0.001855     -0.056337        1.000000   \n",
      "mqtt_qos                  -0.024469  0.988170      0.228114       -0.044270   \n",
      "mqtt_ver                   0.552715  0.001855     -0.056337        1.000000   \n",
      "\n",
      "                        mqtt_qos  mqtt_ver  \n",
      "tcp_time_delta         -0.037996 -0.009565  \n",
      "tcp_len                 0.271391 -0.013370  \n",
      "mqtt_conflag_cleansess -0.044270  1.000000  \n",
      "mqtt_conflag_passwd    -0.029170  0.658921  \n",
      "mqtt_conflag_uname     -0.029213  0.659891  \n",
      "mqtt_dupflag            0.550238 -0.024359  \n",
      "mqtt_kalive            -0.024469  0.552715  \n",
      "mqtt_len                0.988170  0.001855  \n",
      "mqtt_msgtype            0.228114 -0.056337  \n",
      "mqtt_proto_len         -0.044270  1.000000  \n",
      "mqtt_qos                1.000000 -0.044270  \n",
      "mqtt_ver               -0.044270  1.000000  \n"
     ]
    }
   ],
   "source": [
    "# deal with correlations\n",
    "correlation_matrix = df_read_underscore_dropped.toPandas().corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_underscore = ['tcp_flags','tcp_time_delta','tcp_len','mqtt_conack_flags','mqtt_conack_flags_reserved',\n",
    "             'mqtt_conack_flags_sp','mqtt_conack_val','mqtt_conflag_cleansess','mqtt_conflag_passwd',\n",
    "             'mqtt_conflag_qos','mqtt_conflag_reserved','mqtt_conflag_retain','mqtt_conflag_uname',\n",
    "             'mqtt_conflag_willflag','mqtt_conflags','mqtt_dupflag','mqtt_hdrflags','mqtt_kalive',\n",
    "             'mqtt_len','mqtt_msg','mqtt_msgid','mqtt_msgtype','mqtt_proto_len','mqtt_protoname',\n",
    "             'mqtt_qos','mqtt_retain','mqtt_sub_qos','mqtt_suback_qos','mqtt_ver','mqtt_willmsg',\n",
    "             'mqtt_willmsg_len','mqtt_willtopic','mqtt_willtopic_len','target'] # 'data_category' removed\n",
    "\n",
    "\n",
    "nominal_cols = ['tcp_flags','mqtt_conflags','mqtt_hdrflags', 'mqtt_protoname']\n",
    "\n",
    "continuous_cols = ['tcp_time_delta', 'tcp_len','mqtt_kalive','mqtt_len',  'mqtt_msgtype',\n",
    "                  'mqtt_proto_len','mqtt_qos', 'mqtt_ver']\n",
    "\n",
    "binary_cols = ['mqtt_conflag_cleansess', 'mqtt_conflag_passwd','mqtt_conflag_uname', 'mqtt_dupflag']\n",
    "\n",
    "\n",
    "keys = ['slowite', 'bruteforce','flood', 'malformed', 'dos', 'legitimate']\n",
    "vals = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "global label_dict\n",
    "label_dict = dict(zip(keys, vals))\n",
    "\n",
    "# ===========================================================================\n",
    "\n",
    "class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        label_to_multiclass = udf(lambda name: label_dict[name])\n",
    "        output_df = dataset.withColumn('outcome', label_to_multiclass(col('target'))).drop('target')\n",
    "        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "        return output_df\n",
    "\n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in binary_cols + continuous_cols:\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
    "\n",
    "        return output_df\n",
    "    \n",
    "class ColumnDropper(Transformer): # this transformer drops uannecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "def get_preprocess_pipeline():\n",
    "    # Stage where columns are casted as appropriate types\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n",
    "    \n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "    \n",
    "    # Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "    feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n",
    "    corelated_cols_to_remove = ['mqtt_conflag_uname','mqtt_qos','mqtt_proto_len', 'mqtt_ver']\n",
    "\n",
    "    for col_name in corelated_cols_to_remove:\n",
    "        feature_cols.remove(col_name)\n",
    "    \n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "    \n",
    "\n",
    "    # Stage for creating the outcome column representing whether there is attack \n",
    "    stage_outcome = OutcomeCreater()\n",
    "\n",
    "    # Removing all unnecessary columns, only keeping the 'features' and 'outcome' columns\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols+nominal_id_cols+\n",
    "        nominal_onehot_cols+ binary_cols + continuous_cols + ['vectorized_features'])\n",
    "     \n",
    "        \n",
    "    # fit with logistic regression\n",
    "    lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome', maxIter=10)\n",
    "    \n",
    "    # Connect the columns into a pipeline\n",
    "    pipeline = Pipeline(stages=[stage_typecaster,stage_nominal_indexer,stage_nominal_onehot_encoder,\n",
    "        stage_vector_assembler,stage_scaler,stage_outcome,stage_column_dropper])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# fit and transform\n",
    "preprocess_pipeline = get_preprocess_pipeline()\n",
    "preprocess_pipeline_model = preprocess_pipeline.fit(df_read_train)\n",
    "\n",
    "train_transform = preprocess_pipeline_model.transform(df_read_train)\n",
    "test_transform = preprocess_pipeline_model.transform(df_read_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning - PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers chosen are: Logistic Regression & Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/28 18:48:15 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/11/28 18:48:15 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome', maxIter=10)\n",
    "lr_fit = lr.fit(train_transform)\n",
    "\n",
    "lr_preds_train = lr_fit.transform(train_transform)\n",
    "lr_preds_test = lr_fit.transform(test_transform)\n",
    "\n",
    "# random forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'outcome')\n",
    "rf_fit = rf.fit(train_transform)\n",
    "\n",
    "rf_preds_train = rf_fit.transform(train_transform)\n",
    "rf_preds_test = rf_fit.transform(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Train Accuracy : 0.8226690726366956\n",
      "Test Accuracy : 0.8221123638131143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest\n",
      "Train Accuracy : 0.8465114873557066\n",
      "Test Accuracy : 0.8577254567225048\n"
     ]
    }
   ],
   "source": [
    "# accuracies\n",
    "\n",
    "# logistic regression\n",
    "lr_accuracy_train = (lr_preds_train.filter(lr_preds_train.outcome == lr_preds_train.prediction)\n",
    "    .count() / float(lr_preds_train.count()))\n",
    "\n",
    "lr_accuracy_test = (lr_preds_test.filter(lr_preds_test.outcome == lr_preds_test.prediction)\n",
    "    .count() / float(lr_preds_test.count()))\n",
    "\n",
    "print('Logistic Regression')\n",
    "print(\"Train Accuracy :\", lr_accuracy_train)\n",
    "print(\"Test Accuracy :\", lr_accuracy_test)\n",
    "\n",
    "\n",
    "# rf\n",
    "rf_accuracy_train = (rf_preds_train.filter(rf_preds_train.outcome == rf_preds_train.prediction)\n",
    "    .count() / float(rf_preds_train.count()))\n",
    "\n",
    "rf_accuracy_test = (rf_preds_test.filter(rf_preds_test.outcome == rf_preds_test.prediction)\n",
    "    .count() / float(rf_preds_test.count()))\n",
    "\n",
    "print('\\nRandom Forest')\n",
    "print(\"Train Accuracy :\", rf_accuracy_train)\n",
    "print(\"Test Accuracy :\", rf_accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome')\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "lr_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.0001, 1.0])\n",
    "             .addGrid(lr.maxIter, [10, 50])\n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', \n",
    "    labelCol='outcome', metricName='accuracy')\n",
    "\n",
    "lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=lr_paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "lr_cv_fit_train = lr_cv.fit(train_transform)\n",
    "lr_cv_preds_test = lr_cv_fit_train.transform(test_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'outcome')\n",
    "\n",
    "rf_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [10, 15])# maximum depth for each tree\n",
    "             .addGrid(rf.numTrees,[30, 60])# number of trues\n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', \n",
    "    labelCol='outcome', metricName='accuracy')\n",
    "\n",
    "rf_cv = CrossValidator(estimator=rf, estimatorParamMaps=rf_paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "rf_cv_fit_train = rf_cv.fit(train_transform)\n",
    "\n",
    "rf_cv_preds_test = rf_cv_fit_train.transform(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression\n",
      "Pre-CV: 0.8221123638131143\n",
      "Post-CV: 0.8269848608789463\n",
      "\n",
      "Random Forest\n",
      "Pre-CV: 0.8577254567225048\n",
      "Post-CV: 0.9002620004504985\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "lr_accuracy_test_cv = (lr_cv_preds_test.filter(lr_cv_preds_test.outcome == lr_cv_preds_test.prediction)\n",
    "    .count() / float(lr_cv_preds_test.count()))\n",
    "\n",
    "# random forest\n",
    "rf_accuracy_test_cv = (rf_cv_preds_test.filter(rf_cv_preds_test.outcome == rf_cv_preds_test.prediction)\n",
    "    .count() / float(rf_cv_preds_test.count()))\n",
    "\n",
    "print('\\nLogistic Regression')\n",
    "print(\"Pre-CV:\", lr_accuracy_test)\n",
    "print(\"Post-CV:\", lr_accuracy_test_cv)\n",
    "\n",
    "\n",
    "print(\"\\nRandom Forest\")\n",
    "print(\"Pre-CV:\", rf_accuracy_test)\n",
    "print(\"Post-CV:\", rf_accuracy_test_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning - TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two classifiers chosen are: a shallow NN and a deep NN. The shallow NN only has 2 hidden layers, while the deep NN has 5 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # now import the tensorflow module\n",
    "print(tf.__version__)  # make sure the version is 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 19:06:12.438094: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-28 19:06:12.440566: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# create tensors\n",
    "\n",
    "x_train = tf.constant(np.array(train_transform.toPandas()['features'].values.tolist()))\n",
    "y_train = tf.constant(np.array(train_transform.toPandas()['outcome'].values.tolist()))\n",
    "\n",
    "x_test = tf.constant(np.array(test_transform.toPandas()['features'].values.tolist()))\n",
    "y_test = tf.constant(np.array(test_transform.toPandas()['outcome'].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 231646 samples\n",
      "Epoch 1/10\n",
      "231646/231646 - 7s - loss: 0.4887 - sparse_categorical_accuracy: 0.8021\n",
      "Epoch 2/10\n",
      "231646/231646 - 6s - loss: 0.4392 - sparse_categorical_accuracy: 0.8165\n",
      "Epoch 3/10\n",
      "231646/231646 - 6s - loss: 0.4336 - sparse_categorical_accuracy: 0.8196\n",
      "Epoch 4/10\n",
      "231646/231646 - 6s - loss: 0.4306 - sparse_categorical_accuracy: 0.8220\n",
      "Epoch 5/10\n",
      "231646/231646 - 6s - loss: 0.4281 - sparse_categorical_accuracy: 0.8241\n",
      "Epoch 6/10\n",
      "231646/231646 - 6s - loss: 0.4255 - sparse_categorical_accuracy: 0.8256\n",
      "Epoch 7/10\n",
      "231646/231646 - 5s - loss: 0.4233 - sparse_categorical_accuracy: 0.8255\n",
      "Epoch 8/10\n",
      "231646/231646 - 5s - loss: 0.4220 - sparse_categorical_accuracy: 0.8259\n",
      "Epoch 9/10\n",
      "231646/231646 - 6s - loss: 0.4214 - sparse_categorical_accuracy: 0.8259\n",
      "Epoch 10/10\n",
      "231646/231646 - 6s - loss: 0.4209 - sparse_categorical_accuracy: 0.8259\n",
      "84351/1 - 2s - loss: 0.3929 - sparse_categorical_accuracy: 0.8269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4318711336305647, 0.82690185]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shallow NN (2 hidden layers)\n",
    "from tensorflow import keras\n",
    "\n",
    "model_shallow = keras.Sequential()\n",
    "\n",
    "model_shallow.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "model_shallow.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "\n",
    "model_shallow.add(keras.layers.Dense(6))\n",
    "\n",
    "model_shallow.compile(optimizer = keras.optimizers.SGD(), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model_shallow.fit(x_train, y_train, epochs = 10, verbose = 2)\n",
    "model_shallow.evaluate(x_test, y_test, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 231646 samples\n",
      "Epoch 1/10\n",
      "231646/231646 - 8s - loss: 0.5080 - sparse_categorical_accuracy: 0.7954\n",
      "Epoch 2/10\n",
      "231646/231646 - 7s - loss: 0.4386 - sparse_categorical_accuracy: 0.8168\n",
      "Epoch 3/10\n",
      "231646/231646 - 8s - loss: 0.4300 - sparse_categorical_accuracy: 0.8222\n",
      "Epoch 4/10\n",
      "231646/231646 - 8s - loss: 0.4270 - sparse_categorical_accuracy: 0.8240\n",
      "Epoch 5/10\n",
      "231646/231646 - 7s - loss: 0.4254 - sparse_categorical_accuracy: 0.8243\n",
      "Epoch 6/10\n",
      "231646/231646 - 8s - loss: 0.4246 - sparse_categorical_accuracy: 0.8245\n",
      "Epoch 7/10\n",
      "231646/231646 - 7s - loss: 0.4237 - sparse_categorical_accuracy: 0.8249\n",
      "Epoch 8/10\n",
      "231646/231646 - 7s - loss: 0.4232 - sparse_categorical_accuracy: 0.8256\n",
      "Epoch 9/10\n",
      "231646/231646 - 7s - loss: 0.4227 - sparse_categorical_accuracy: 0.8255\n",
      "Epoch 10/10\n",
      "231646/231646 - 7s - loss: 0.4223 - sparse_categorical_accuracy: 0.8258\n",
      "84351/1 - 2s - loss: 0.3922 - sparse_categorical_accuracy: 0.8224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42987401769280814, 0.82236135]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model_deep = keras.Sequential()\n",
    "\n",
    "model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "\n",
    "\n",
    "model_deep.add(keras.layers.Dense(6))\n",
    "\n",
    "model_deep.compile(optimizer = keras.optimizers.SGD(), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model_deep.fit(x_train, y_train, epochs = 10, verbose = 2)\n",
    "model_deep.evaluate(x_test, y_test, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the x and y train tensors and shuffle\n",
    "tf_train = tf.concat([x_train, tf.reshape(y_train, [-1, 1])], axis = 1)\n",
    "tf_train_shuffle = tf.random.shuffle(tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_cross_val_activation_width(k, act_fun, width):\n",
    "    \n",
    "    cuts = np.linspace(0, tf_train_shuffle.shape[0]-1, k+1, dtype = int)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    model.add(keras.layers.Dense(6))\n",
    "\n",
    "    metric = []\n",
    "\n",
    "    for i in range(k):\n",
    "        print('k = '+str(i+1)+'\\n')\n",
    "        \n",
    "        val = tf_train_shuffle[cuts[i]:cuts[i+1]]\n",
    "            \n",
    "        mask = np.ones(tf_train_shuffle.shape[0])\n",
    "        mask[cuts[i]:cuts[i+1]] = 0\n",
    "        \n",
    "        train = tf.boolean_mask(tf_train_shuffle, mask)\n",
    "    \n",
    "        cur_x_train = train[:,:-1]\n",
    "        cur_y_train = train[:,-1]\n",
    "        \n",
    "        cur_x_val = val[:,:-1]\n",
    "        cur_y_val = val[:,-1]\n",
    "        \n",
    "        model.compile(optimizer = keras.optimizers.SGD(), \n",
    "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "        \n",
    "        fit_data = model.fit(cur_x_train, cur_y_train, epochs = 5, \n",
    "                             verbose = 2,validation_data = (cur_x_val, cur_y_val))\n",
    "        \n",
    "        cur_auc = np.mean(fit_data.history['val_sparse_categorical_accuracy'])\n",
    "        metric.append(cur_auc)\n",
    "        \n",
    "        if np.max(metric) == cur_auc:\n",
    "            best_model = model\n",
    "            print('\\nNew best model saved.')\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "    print(metric)\n",
    "    return np.mean(metric), best_model.evaluate(x_test, y_test, verbose = 2), best_model\n",
    "\n",
    "\n",
    "# val_acc, test_res, best_model_shallow = shallow_cross_val_activation_width(k = 4, act_fun = 'relu', width = 5)\n",
    "\n",
    "# print('\\n===========')\n",
    "# print(\"Validation Accuracy:\", val_acc)\n",
    "# print(\"Test Accuracy:\", test_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_cross_val_activation_width(k, act_fun, width):\n",
    "    \n",
    "    cuts = np.linspace(0, tf_train_shuffle.shape[0]-1, k+1, dtype = int)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    \n",
    "    model.add(keras.layers.Dense(6))\n",
    "\n",
    "    metric = []\n",
    "\n",
    "    for i in range(k):\n",
    "        print('k = '+str(i+1)+'\\n')\n",
    "        \n",
    "        val = tf_train_shuffle[cuts[i]:cuts[i+1]]\n",
    "            \n",
    "        mask = np.ones(tf_train_shuffle.shape[0])\n",
    "        mask[cuts[i]:cuts[i+1]] = 0\n",
    "        \n",
    "        train = tf.boolean_mask(tf_train_shuffle, mask)\n",
    "    \n",
    "        cur_x_train = train[:,:-1]\n",
    "        cur_y_train = train[:,-1]\n",
    "        \n",
    "        cur_x_val = val[:,:-1]\n",
    "        cur_y_val = val[:,-1]\n",
    "        \n",
    "        model.compile(optimizer = keras.optimizers.SGD(), \n",
    "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "        \n",
    "        fit_data = model.fit(cur_x_train, cur_y_train, epochs = 5,\n",
    "                             verbose = 2, validation_data = (cur_x_val, cur_y_val))\n",
    "        \n",
    "        cur_auc = np.mean(fit_data.history['val_sparse_categorical_accuracy'])\n",
    "        metric.append(cur_auc)\n",
    "        \n",
    "                \n",
    "        if np.max(metric) == cur_auc:\n",
    "            best_model = model\n",
    "            print('\\nNew best model saved.')\n",
    "\n",
    "        print('\\n')\n",
    "        \n",
    "    print(metric)\n",
    "    return np.mean(metric), best_model.evaluate(x_train, y_train, verbose = 2), best_model\n",
    "\n",
    "\n",
    "# val_acc, test_res, best_model_deep = deep_cross_val_activation_width(k = 4, act_fun = 'relu', width = 5)\n",
    "\n",
    "# print('\\n===========')\n",
    "# print(\"Validation Accuracy:\", val_acc)\n",
    "# print(\"Test Accuracy:\", test_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting trial: run-relu width10\n",
      "k = 1\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 6s - loss: 0.5053 - sparse_categorical_accuracy: 0.7963 - val_loss: 0.4550 - val_sparse_categorical_accuracy: 0.8218\n",
      "Epoch 2/5\n",
      "154431/154431 - 6s - loss: 0.4498 - sparse_categorical_accuracy: 0.8142 - val_loss: 0.4388 - val_sparse_categorical_accuracy: 0.8240\n",
      "Epoch 3/5\n",
      "154431/154431 - 5s - loss: 0.4440 - sparse_categorical_accuracy: 0.8164 - val_loss: 0.4450 - val_sparse_categorical_accuracy: 0.8240\n",
      "Epoch 4/5\n",
      "154431/154431 - 6s - loss: 0.4395 - sparse_categorical_accuracy: 0.8195 - val_loss: 0.4382 - val_sparse_categorical_accuracy: 0.8270\n",
      "Epoch 5/5\n",
      "154431/154431 - 6s - loss: 0.4359 - sparse_categorical_accuracy: 0.8225 - val_loss: 0.4330 - val_sparse_categorical_accuracy: 0.8270\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 6s - loss: 0.4325 - sparse_categorical_accuracy: 0.8240 - val_loss: 0.4288 - val_sparse_categorical_accuracy: 0.8260\n",
      "Epoch 2/5\n",
      "154431/154431 - 5s - loss: 0.4306 - sparse_categorical_accuracy: 0.8248 - val_loss: 0.4261 - val_sparse_categorical_accuracy: 0.8260\n",
      "Epoch 3/5\n",
      "154431/154431 - 5s - loss: 0.4288 - sparse_categorical_accuracy: 0.8257 - val_loss: 0.4249 - val_sparse_categorical_accuracy: 0.8254\n",
      "Epoch 4/5\n",
      "154431/154431 - 5s - loss: 0.4274 - sparse_categorical_accuracy: 0.8259 - val_loss: 0.4302 - val_sparse_categorical_accuracy: 0.8260\n",
      "Epoch 5/5\n",
      "154431/154431 - 5s - loss: 0.4263 - sparse_categorical_accuracy: 0.8256 - val_loss: 0.4269 - val_sparse_categorical_accuracy: 0.8260\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 6s - loss: 0.4240 - sparse_categorical_accuracy: 0.8261 - val_loss: 0.4262 - val_sparse_categorical_accuracy: 0.8250\n",
      "Epoch 2/5\n",
      "154431/154431 - 7s - loss: 0.4230 - sparse_categorical_accuracy: 0.8259 - val_loss: 0.4228 - val_sparse_categorical_accuracy: 0.8250\n",
      "Epoch 3/5\n",
      "154431/154431 - 5s - loss: 0.4224 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.4220 - val_sparse_categorical_accuracy: 0.8251\n",
      "Epoch 4/5\n",
      "154431/154431 - 6s - loss: 0.4220 - sparse_categorical_accuracy: 0.8259 - val_loss: 0.4240 - val_sparse_categorical_accuracy: 0.8258\n",
      "Epoch 5/5\n",
      "154431/154431 - 5s - loss: 0.4216 - sparse_categorical_accuracy: 0.8259 - val_loss: 0.4225 - val_sparse_categorical_accuracy: 0.8258\n",
      "\n",
      "\n",
      "[0.8247646, 0.825881, 0.8253423]\n",
      "84351/1 - 1s - loss: 0.3871 - sparse_categorical_accuracy: 0.8270\n",
      "New Best Model: run-relu width10\n",
      "\n",
      "--- Starting trial: run-relu width20\n",
      "k = 1\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 6s - loss: 0.4789 - sparse_categorical_accuracy: 0.8054 - val_loss: 0.4603 - val_sparse_categorical_accuracy: 0.8224\n",
      "Epoch 2/5\n",
      "154431/154431 - 5s - loss: 0.4375 - sparse_categorical_accuracy: 0.8161 - val_loss: 0.4279 - val_sparse_categorical_accuracy: 0.8244\n",
      "Epoch 3/5\n",
      "154431/154431 - 6s - loss: 0.4292 - sparse_categorical_accuracy: 0.8211 - val_loss: 0.4351 - val_sparse_categorical_accuracy: 0.8270\n",
      "Epoch 4/5\n",
      "154431/154431 - 5s - loss: 0.4254 - sparse_categorical_accuracy: 0.8234 - val_loss: 0.4207 - val_sparse_categorical_accuracy: 0.8270\n",
      "Epoch 5/5\n",
      "154431/154431 - 6s - loss: 0.4238 - sparse_categorical_accuracy: 0.8253 - val_loss: 0.4187 - val_sparse_categorical_accuracy: 0.8270\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 6s - loss: 0.4224 - sparse_categorical_accuracy: 0.8259 - val_loss: 0.4195 - val_sparse_categorical_accuracy: 0.8255\n",
      "Epoch 2/5\n",
      "154431/154431 - 5s - loss: 0.4216 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.4203 - val_sparse_categorical_accuracy: 0.8260\n",
      "Epoch 3/5\n",
      "154431/154431 - 6s - loss: 0.4212 - sparse_categorical_accuracy: 0.8261 - val_loss: 0.4189 - val_sparse_categorical_accuracy: 0.8246\n",
      "Epoch 4/5\n",
      "154431/154431 - 5s - loss: 0.4210 - sparse_categorical_accuracy: 0.8262 - val_loss: 0.4282 - val_sparse_categorical_accuracy: 0.8255\n",
      "Epoch 5/5\n",
      "154431/154431 - 6s - loss: 0.4208 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.4180 - val_sparse_categorical_accuracy: 0.8260\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 6s - loss: 0.4196 - sparse_categorical_accuracy: 0.8261 - val_loss: 0.4210 - val_sparse_categorical_accuracy: 0.8232\n",
      "Epoch 2/5\n",
      "154431/154431 - 7s - loss: 0.4191 - sparse_categorical_accuracy: 0.8263 - val_loss: 0.4221 - val_sparse_categorical_accuracy: 0.8249\n",
      "Epoch 3/5\n",
      "154431/154431 - 6s - loss: 0.4190 - sparse_categorical_accuracy: 0.8261 - val_loss: 0.4204 - val_sparse_categorical_accuracy: 0.8258\n",
      "Epoch 4/5\n",
      "154431/154431 - 5s - loss: 0.4187 - sparse_categorical_accuracy: 0.8261 - val_loss: 0.4211 - val_sparse_categorical_accuracy: 0.8258\n",
      "Epoch 5/5\n",
      "154431/154431 - 5s - loss: 0.4189 - sparse_categorical_accuracy: 0.8261 - val_loss: 0.4214 - val_sparse_categorical_accuracy: 0.8258\n",
      "\n",
      "\n",
      "[0.825565, 0.8255261, 0.8251246]\n",
      "84351/1 - 1s - loss: 0.3883 - sparse_categorical_accuracy: 0.8271\n",
      "New Best Model: run-relu width20\n",
      "\n",
      "--- Starting trial: run-softmax width10\n",
      "k = 1\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 8s - loss: 1.1229 - sparse_categorical_accuracy: 0.5010 - val_loss: 1.0699 - val_sparse_categorical_accuracy: 0.4980\n",
      "Epoch 2/5\n",
      "154431/154431 - 7s - loss: 1.0002 - sparse_categorical_accuracy: 0.6161 - val_loss: 0.8081 - val_sparse_categorical_accuracy: 0.7787\n",
      "Epoch 3/5\n",
      "154431/154431 - 7s - loss: 0.7030 - sparse_categorical_accuracy: 0.7706 - val_loss: 0.6342 - val_sparse_categorical_accuracy: 0.7811\n",
      "Epoch 4/5\n",
      "154431/154431 - 7s - loss: 0.6151 - sparse_categorical_accuracy: 0.7797 - val_loss: 0.5891 - val_sparse_categorical_accuracy: 0.7832\n",
      "Epoch 5/5\n",
      "154431/154431 - 7s - loss: 0.5817 - sparse_categorical_accuracy: 0.7817 - val_loss: 0.5698 - val_sparse_categorical_accuracy: 0.7832\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 8s - loss: 0.5674 - sparse_categorical_accuracy: 0.7811 - val_loss: 0.5551 - val_sparse_categorical_accuracy: 0.7827\n",
      "Epoch 2/5\n",
      "154431/154431 - 7s - loss: 0.5508 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.5363 - val_sparse_categorical_accuracy: 0.7827\n",
      "Epoch 3/5\n",
      "154431/154431 - 7s - loss: 0.5300 - sparse_categorical_accuracy: 0.7870 - val_loss: 0.5234 - val_sparse_categorical_accuracy: 0.8007\n",
      "Epoch 4/5\n",
      "154431/154431 - 7s - loss: 0.5136 - sparse_categorical_accuracy: 0.8004 - val_loss: 0.5049 - val_sparse_categorical_accuracy: 0.8007\n",
      "Epoch 5/5\n",
      "154431/154431 - 7s - loss: 0.5038 - sparse_categorical_accuracy: 0.8049 - val_loss: 0.4977 - val_sparse_categorical_accuracy: 0.8052\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 8s - loss: 0.4967 - sparse_categorical_accuracy: 0.8054 - val_loss: 0.4963 - val_sparse_categorical_accuracy: 0.8051\n",
      "Epoch 2/5\n",
      "154431/154431 - 7s - loss: 0.4930 - sparse_categorical_accuracy: 0.8055 - val_loss: 0.4996 - val_sparse_categorical_accuracy: 0.8051\n",
      "Epoch 3/5\n",
      "154431/154431 - 7s - loss: 0.4904 - sparse_categorical_accuracy: 0.8051 - val_loss: 0.4919 - val_sparse_categorical_accuracy: 0.8051\n",
      "Epoch 4/5\n",
      "154431/154431 - 7s - loss: 0.4884 - sparse_categorical_accuracy: 0.8052 - val_loss: 0.4892 - val_sparse_categorical_accuracy: 0.8051\n",
      "Epoch 5/5\n",
      "154431/154431 - 7s - loss: 0.4875 - sparse_categorical_accuracy: 0.8053 - val_loss: 0.4889 - val_sparse_categorical_accuracy: 0.8051\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "[0.7248514, 0.79440784, 0.80508196]\n",
      "84351/1 - 2s - loss: 0.4696 - sparse_categorical_accuracy: 0.8032\n",
      "\n",
      "--- Starting trial: run-softmax width20\n",
      "k = 1\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 8s - loss: 1.1351 - sparse_categorical_accuracy: 0.5002 - val_loss: 1.0808 - val_sparse_categorical_accuracy: 0.4980\n",
      "Epoch 2/5\n",
      "154431/154431 - 7s - loss: 1.0798 - sparse_categorical_accuracy: 0.5010 - val_loss: 1.0762 - val_sparse_categorical_accuracy: 0.4980\n",
      "Epoch 3/5\n",
      "154431/154431 - 7s - loss: 1.0751 - sparse_categorical_accuracy: 0.5010 - val_loss: 1.0690 - val_sparse_categorical_accuracy: 0.4980\n",
      "Epoch 4/5\n",
      "154431/154431 - 8s - loss: 1.0541 - sparse_categorical_accuracy: 0.5176 - val_loss: 1.0137 - val_sparse_categorical_accuracy: 0.7831\n",
      "Epoch 5/5\n",
      "154431/154431 - 9s - loss: 0.8397 - sparse_categorical_accuracy: 0.7812 - val_loss: 0.7049 - val_sparse_categorical_accuracy: 0.7830\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 10s - loss: 0.6785 - sparse_categorical_accuracy: 0.7748 - val_loss: 0.6456 - val_sparse_categorical_accuracy: 0.7825\n",
      "Epoch 2/5\n",
      "154431/154431 - 9s - loss: 0.6241 - sparse_categorical_accuracy: 0.7817 - val_loss: 0.6122 - val_sparse_categorical_accuracy: 0.7825\n",
      "Epoch 3/5\n",
      "154431/154431 - 9s - loss: 0.6022 - sparse_categorical_accuracy: 0.7824 - val_loss: 0.5949 - val_sparse_categorical_accuracy: 0.7825\n",
      "Epoch 4/5\n",
      "154431/154431 - 8s - loss: 0.5913 - sparse_categorical_accuracy: 0.7824 - val_loss: 0.5879 - val_sparse_categorical_accuracy: 0.7825\n",
      "Epoch 5/5\n",
      "154431/154431 - 9s - loss: 0.5847 - sparse_categorical_accuracy: 0.7824 - val_loss: 0.5823 - val_sparse_categorical_accuracy: 0.7825\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 10s - loss: 0.5795 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.5804 - val_sparse_categorical_accuracy: 0.7819\n",
      "Epoch 2/5\n",
      "154431/154431 - 8s - loss: 0.5767 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.5786 - val_sparse_categorical_accuracy: 0.7819\n",
      "Epoch 3/5\n",
      "154431/154431 - 8s - loss: 0.5748 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.5798 - val_sparse_categorical_accuracy: 0.7817\n",
      "Epoch 4/5\n",
      "154431/154431 - 9s - loss: 0.5726 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.5733 - val_sparse_categorical_accuracy: 0.7819\n",
      "Epoch 5/5\n",
      "154431/154431 - 9s - loss: 0.5713 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.5762 - val_sparse_categorical_accuracy: 0.7819\n",
      "\n",
      "\n",
      "[0.6120106, 0.78245157, 0.7818533]\n",
      "84351/1 - 2s - loss: 0.6175 - sparse_categorical_accuracy: 0.7813\n",
      "84351/1 - 1s - loss: 0.3883 - sparse_categorical_accuracy: 0.8271\n",
      "84351/1 - 1s - loss: 0.3883 - sparse_categorical_accuracy: 0.8271\n",
      "Best Model Hyperparameters: run-relu width20\n",
      "Test Accuracy: 0.8271153\n"
     ]
    }
   ],
   "source": [
    "act_funs = ['relu', 'softmax']\n",
    "widths = [10, 20]\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for act_fun in act_funs:\n",
    "    for width in widths:\n",
    "\n",
    "        run_name = 'run-'+act_fun+' width'+str(width)\n",
    "        print('')\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "\n",
    "        run_dir = 'logs14813/hparam_tuning_q2_2/' + run_name\n",
    "        val_acc, test_res, hp_model = shallow_cross_val_activation_width(3, act_fun, width)\n",
    "        \n",
    "        accuracies.append(test_res[1])\n",
    "        \n",
    "        if np.max(accuracies) == test_res[1]:\n",
    "            best_hp_shallow_model = hp_model\n",
    "            best_shallow_run_name = run_name\n",
    "            print('New Best Model:', best_shallow_run_name)\n",
    "            \n",
    "best_hp_shallow_model.evaluate(x_test, y_test, verbose = 2)\n",
    "shallow_res = best_hp_shallow_model.evaluate(x_test, y_test, verbose = 2)\n",
    "\n",
    "print('Best Model Hyperparameters:', best_shallow_run_name)\n",
    "print('Test Accuracy:', shallow_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting trial: run-relu width10\n",
      "k = 1\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 7s - loss: 0.5147 - sparse_categorical_accuracy: 0.7957 - val_loss: 0.4506 - val_sparse_categorical_accuracy: 0.8205\n",
      "Epoch 2/5\n",
      "154431/154431 - 7s - loss: 0.4453 - sparse_categorical_accuracy: 0.8198 - val_loss: 0.4367 - val_sparse_categorical_accuracy: 0.8263\n",
      "Epoch 3/5\n",
      "154431/154431 - 6s - loss: 0.4376 - sparse_categorical_accuracy: 0.8230 - val_loss: 0.4342 - val_sparse_categorical_accuracy: 0.8236\n",
      "Epoch 4/5\n",
      "154431/154431 - 6s - loss: 0.4344 - sparse_categorical_accuracy: 0.8237 - val_loss: 0.4268 - val_sparse_categorical_accuracy: 0.8239\n",
      "Epoch 5/5\n",
      "154431/154431 - 6s - loss: 0.4315 - sparse_categorical_accuracy: 0.8240 - val_loss: 0.4275 - val_sparse_categorical_accuracy: 0.8263\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 7s - loss: 0.4286 - sparse_categorical_accuracy: 0.8248 - val_loss: 0.4248 - val_sparse_categorical_accuracy: 0.8252\n",
      "Epoch 2/5\n",
      "154431/154431 - 6s - loss: 0.4271 - sparse_categorical_accuracy: 0.8254 - val_loss: 0.4234 - val_sparse_categorical_accuracy: 0.8260\n",
      "Epoch 3/5\n",
      "154431/154431 - 7s - loss: 0.4257 - sparse_categorical_accuracy: 0.8258 - val_loss: 0.4225 - val_sparse_categorical_accuracy: 0.8260\n",
      "Epoch 4/5\n",
      "154431/154431 - 7s - loss: 0.4239 - sparse_categorical_accuracy: 0.8256 - val_loss: 0.4222 - val_sparse_categorical_accuracy: 0.8260\n",
      "Epoch 5/5\n",
      "154431/154431 - 6s - loss: 0.4231 - sparse_categorical_accuracy: 0.8255 - val_loss: 0.4224 - val_sparse_categorical_accuracy: 0.8260\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 7s - loss: 0.4215 - sparse_categorical_accuracy: 0.8258 - val_loss: 0.4229 - val_sparse_categorical_accuracy: 0.8251\n",
      "Epoch 2/5\n",
      "154431/154431 - 6s - loss: 0.4211 - sparse_categorical_accuracy: 0.8259 - val_loss: 0.4219 - val_sparse_categorical_accuracy: 0.8258\n",
      "Epoch 3/5\n",
      "154431/154431 - 6s - loss: 0.4214 - sparse_categorical_accuracy: 0.8261 - val_loss: 0.4221 - val_sparse_categorical_accuracy: 0.8251\n",
      "Epoch 4/5\n",
      "154431/154431 - 6s - loss: 0.4204 - sparse_categorical_accuracy: 0.8261 - val_loss: 0.4229 - val_sparse_categorical_accuracy: 0.8258\n",
      "Epoch 5/5\n",
      "154431/154431 - 6s - loss: 0.4203 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.4230 - val_sparse_categorical_accuracy: 0.8258\n",
      "\n",
      "\n",
      "[0.8241118, 0.8258422, 0.82551575]\n",
      "231646/1 - 5s - loss: 0.4511 - sparse_categorical_accuracy: 0.8263\n",
      "New Best Model: run-relu width10\n",
      "\n",
      "--- Starting trial: run-relu width20\n",
      "k = 1\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 7s - loss: 0.5097 - sparse_categorical_accuracy: 0.7968 - val_loss: 0.4595 - val_sparse_categorical_accuracy: 0.7501\n",
      "Epoch 2/5\n",
      "154431/154431 - 6s - loss: 0.4383 - sparse_categorical_accuracy: 0.8200 - val_loss: 0.4324 - val_sparse_categorical_accuracy: 0.8234\n",
      "Epoch 3/5\n",
      "154431/154431 - 6s - loss: 0.4293 - sparse_categorical_accuracy: 0.8236 - val_loss: 0.4283 - val_sparse_categorical_accuracy: 0.8257\n",
      "Epoch 4/5\n",
      "154431/154431 - 6s - loss: 0.4268 - sparse_categorical_accuracy: 0.8241 - val_loss: 0.4236 - val_sparse_categorical_accuracy: 0.8263\n",
      "Epoch 5/5\n",
      "154431/154431 - 6s - loss: 0.4250 - sparse_categorical_accuracy: 0.8252 - val_loss: 0.4220 - val_sparse_categorical_accuracy: 0.8270\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 7s - loss: 0.4233 - sparse_categorical_accuracy: 0.8255 - val_loss: 0.4220 - val_sparse_categorical_accuracy: 0.8254\n",
      "Epoch 2/5\n",
      "154431/154431 - 7s - loss: 0.4225 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.4197 - val_sparse_categorical_accuracy: 0.8254\n",
      "Epoch 3/5\n",
      "154431/154431 - 6s - loss: 0.4217 - sparse_categorical_accuracy: 0.8255 - val_loss: 0.4251 - val_sparse_categorical_accuracy: 0.8241\n",
      "Epoch 4/5\n",
      "154431/154431 - 6s - loss: 0.4214 - sparse_categorical_accuracy: 0.8257 - val_loss: 0.4186 - val_sparse_categorical_accuracy: 0.8260\n",
      "Epoch 5/5\n",
      "154431/154431 - 6s - loss: 0.4209 - sparse_categorical_accuracy: 0.8255 - val_loss: 0.4193 - val_sparse_categorical_accuracy: 0.8260\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 7s - loss: 0.4196 - sparse_categorical_accuracy: 0.8258 - val_loss: 0.4209 - val_sparse_categorical_accuracy: 0.8249\n",
      "Epoch 2/5\n",
      "154431/154431 - 6s - loss: 0.4192 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.4236 - val_sparse_categorical_accuracy: 0.8258\n",
      "Epoch 3/5\n",
      "154431/154431 - 6s - loss: 0.4191 - sparse_categorical_accuracy: 0.8259 - val_loss: 0.4228 - val_sparse_categorical_accuracy: 0.8251\n",
      "Epoch 4/5\n",
      "154431/154431 - 6s - loss: 0.4188 - sparse_categorical_accuracy: 0.8261 - val_loss: 0.4218 - val_sparse_categorical_accuracy: 0.8249\n",
      "Epoch 5/5\n",
      "154431/154431 - 6s - loss: 0.4188 - sparse_categorical_accuracy: 0.8261 - val_loss: 0.4204 - val_sparse_categorical_accuracy: 0.8239\n",
      "\n",
      "\n",
      "[0.81049025, 0.82539666, 0.82493293]\n",
      "231646/1 - 4s - loss: 0.4387 - sparse_categorical_accuracy: 0.8246\n",
      "\n",
      "--- Starting trial: run-softmax width10\n",
      "k = 1\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 12s - loss: 1.1321 - sparse_categorical_accuracy: 0.4962 - val_loss: 1.0804 - val_sparse_categorical_accuracy: 0.4980\n",
      "Epoch 2/5\n",
      "154431/154431 - 11s - loss: 1.0798 - sparse_categorical_accuracy: 0.5010 - val_loss: 1.0765 - val_sparse_categorical_accuracy: 0.4980\n",
      "Epoch 3/5\n",
      "154431/154431 - 11s - loss: 1.0774 - sparse_categorical_accuracy: 0.5010 - val_loss: 1.0752 - val_sparse_categorical_accuracy: 0.4980\n",
      "Epoch 4/5\n",
      "154431/154431 - 11s - loss: 1.0765 - sparse_categorical_accuracy: 0.5010 - val_loss: 1.0745 - val_sparse_categorical_accuracy: 0.4980\n",
      "Epoch 5/5\n",
      "154431/154431 - 11s - loss: 1.0760 - sparse_categorical_accuracy: 0.5010 - val_loss: 1.0741 - val_sparse_categorical_accuracy: 0.4980\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 12s - loss: 1.0756 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0740 - val_sparse_categorical_accuracy: 0.5021\n",
      "Epoch 2/5\n",
      "154431/154431 - 11s - loss: 1.0754 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0738 - val_sparse_categorical_accuracy: 0.5021\n",
      "Epoch 3/5\n",
      "154431/154431 - 11s - loss: 1.0753 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0738 - val_sparse_categorical_accuracy: 0.5021\n",
      "Epoch 4/5\n",
      "154431/154431 - 11s - loss: 1.0752 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0739 - val_sparse_categorical_accuracy: 0.5021\n",
      "Epoch 5/5\n",
      "154431/154431 - 11s - loss: 1.0752 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0736 - val_sparse_categorical_accuracy: 0.5021\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 12s - loss: 1.0736 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.0767 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "154431/154431 - 11s - loss: 1.0736 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.0766 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "154431/154431 - 11s - loss: 1.0736 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.0766 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "154431/154431 - 11s - loss: 1.0735 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.0767 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "154431/154431 - 11s - loss: 1.0736 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.0766 - val_sparse_categorical_accuracy: 0.5000\n",
      "\n",
      "\n",
      "[0.4979732, 0.5020786, 0.49996763]\n",
      "231646/1 - 8s - loss: 1.0864 - sparse_categorical_accuracy: 0.5000\n",
      "\n",
      "--- Starting trial: run-softmax width20\n",
      "k = 1\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 13s - loss: 1.1338 - sparse_categorical_accuracy: 0.4983 - val_loss: 1.0810 - val_sparse_categorical_accuracy: 0.4980\n",
      "Epoch 2/5\n",
      "154431/154431 - 12s - loss: 1.0802 - sparse_categorical_accuracy: 0.5010 - val_loss: 1.0768 - val_sparse_categorical_accuracy: 0.4980\n",
      "Epoch 3/5\n",
      "154431/154431 - 12s - loss: 1.0777 - sparse_categorical_accuracy: 0.5010 - val_loss: 1.0754 - val_sparse_categorical_accuracy: 0.4980\n",
      "Epoch 4/5\n",
      "154431/154431 - 12s - loss: 1.0766 - sparse_categorical_accuracy: 0.5010 - val_loss: 1.0746 - val_sparse_categorical_accuracy: 0.4980\n",
      "Epoch 5/5\n",
      "154431/154431 - 12s - loss: 1.0760 - sparse_categorical_accuracy: 0.5010 - val_loss: 1.0744 - val_sparse_categorical_accuracy: 0.4980\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 13s - loss: 1.0757 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0741 - val_sparse_categorical_accuracy: 0.5021\n",
      "Epoch 2/5\n",
      "154431/154431 - 12s - loss: 1.0755 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0739 - val_sparse_categorical_accuracy: 0.5021\n",
      "Epoch 3/5\n",
      "154431/154431 - 12s - loss: 1.0753 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0740 - val_sparse_categorical_accuracy: 0.5021\n",
      "Epoch 4/5\n",
      "154431/154431 - 12s - loss: 1.0753 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0737 - val_sparse_categorical_accuracy: 0.5021\n",
      "Epoch 5/5\n",
      "154431/154431 - 12s - loss: 1.0752 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0737 - val_sparse_categorical_accuracy: 0.5021\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Train on 154431 samples, validate on 77215 samples\n",
      "Epoch 1/5\n",
      "154431/154431 - 13s - loss: 1.0736 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.0768 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "154431/154431 - 12s - loss: 1.0736 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.0767 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "154431/154431 - 12s - loss: 1.0736 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.0767 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "154431/154431 - 12s - loss: 1.0736 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.0766 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "154431/154431 - 12s - loss: 1.0735 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.0766 - val_sparse_categorical_accuracy: 0.5000\n",
      "\n",
      "\n",
      "[0.4979732, 0.5020786, 0.49996763]\n",
      "231646/1 - 9s - loss: 1.0868 - sparse_categorical_accuracy: 0.5000\n",
      "84351/1 - 2s - loss: 0.3852 - sparse_categorical_accuracy: 0.8270\n",
      "Best Model Hyperparameters: run-relu width10\n",
      "Test Accuracy: 0.8270441\n"
     ]
    }
   ],
   "source": [
    "act_funs = ['relu', 'softmax']\n",
    "widths = [10, 20]\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for act_fun in act_funs:\n",
    "    for width in widths:\n",
    "\n",
    "        run_name = 'run-'+act_fun+' width'+str(width)\n",
    "        print('')\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        \n",
    "        val_acc, test_res, hp_model = deep_cross_val_activation_width(3, act_fun, width)\n",
    "        \n",
    "        accuracies.append(test_res[1])\n",
    "        \n",
    "        if np.max(accuracies) == test_res[1]:\n",
    "            best_hp_deep_model = hp_model\n",
    "            best_deep_run_name = run_name\n",
    "            print('New Best Model:', best_deep_run_name)\n",
    "\n",
    "\n",
    "deep_res = best_hp_deep_model.evaluate(x_test, y_test, verbose = 2)\n",
    "\n",
    "print('Best Model Hyperparameters:', best_deep_run_name)\n",
    "print('Test Accuracy:', deep_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
