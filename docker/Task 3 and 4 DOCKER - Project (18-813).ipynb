{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables to change:\n",
    "\n",
    "1. jar_file_loc: this is the file of the .jar postgres file. Change path to where it is located\n",
    "2. train_data_path & test_data_path: change to path of train/test csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.10.0 in /opt/conda/lib/python3.10/site-packages (2.10.0)\n",
      "Requirement already satisfied: tensorflow-cpu-aws==2.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.10.0) (2.10.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (3.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (4.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (2.1.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (21.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (1.6.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (3.19.6)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (1.50.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (14.0.6)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (2.10.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (22.11.23)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (1.1.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (1.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (0.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (2.10.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (2.10.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (65.5.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (1.23.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (0.38.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (2.14.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (0.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-cpu-aws==2.10.0->tensorflow==2.10.0) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# install if necessary: tensorflow version needs to be 2.10.x\n",
    "# RESTART Kernel after installation\n",
    "\n",
    "!pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: [\"[Errno 2] The file to load file system plugin from does not exist.: '/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so'\"]\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: cannot open shared object file: No such file or directory']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # now import the tensorflow module\n",
    "print(tf.__version__)  # make sure the version is 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APP Name :GenericAppName\n",
      "Master :local[*]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "jar_file_loc = 'postgresql-42.5.0.jar'\n",
    "\n",
    "# set up Spark\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GenericAppName\") \\\n",
    "    .config('spark.jars', jar_file_loc) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#Access SparkContext from your SparkSession\n",
    "print(\"APP Name :\"+ spark.sparkContext.appName);\n",
    "print(\"Master :\"+ spark.sparkContext.master);\n",
    "\n",
    "sqlContext = SQLContext(spark.sparkContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df_train = spark.read.csv('data_folder/train70_reduced.csv', header = True, inferSchema = True)\n",
    "df_test = spark.read.csv('data_folder/test30_reduced.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item Count\n",
      "\n",
      "Train: 231646\n",
      "Test: 84351\n",
      "Combined: 315997\n"
     ]
    }
   ],
   "source": [
    "# add column to differentiate b/w train and test sets\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "df_train_cat = df_train.withColumn(\"data_category\", lit(\"train\"))\n",
    "df_test_cat = df_test.withColumn(\"data_category\", lit(\"test\"))\n",
    "\n",
    "print('Item Count\\n')\n",
    "print('Train:', df_train_cat.count())\n",
    "print('Test:', df_test_cat.count())\n",
    "\n",
    "\n",
    "# combine dfs\n",
    "df_combined = df_train_cat.union(df_test_cat)\n",
    "print('Combined:', df_combined.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write into postgresql db\n",
    "\n",
    "db_properties={}\n",
    "#update your db username\n",
    "db_properties['username']=\"postgres\"\n",
    "#update your db password\n",
    "db_properties['password']=\"bigdata\"\n",
    "#make sure you got the right port number here\n",
    "db_properties['url']= \"jdbc:postgresql://host.docker.internal/postgres\"\n",
    "#make sure you had the Postgres JAR file in the right location\n",
    "db_properties['driver']=\"org.postgresql.Driver\"\n",
    "db_properties['table']= \"mqtt\"\n",
    "\n",
    "# create df with train data \n",
    "df_combined.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", \"bigdata\")\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item Count from PostgreSQL Read: 315997\n"
     ]
    }
   ],
   "source": [
    "# read db to ensure data has been written in correctly\n",
    "df_read = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", \"bigdata\")\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()\n",
    "\n",
    "print('Item Count from PostgreSQL Read:', df_read.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all . with _ in column names to avoid bugs with having . in col name\n",
    "\n",
    "col_names_underscore = ['tcp_flags','tcp_time_delta','tcp_len','mqtt_conack_flags','mqtt_conack_flags_reserved',\n",
    "             'mqtt_conack_flags_sp','mqtt_conack_val','mqtt_conflag_cleansess','mqtt_conflag_passwd',\n",
    "             'mqtt_conflag_qos','mqtt_conflag_reserved','mqtt_conflag_retain','mqtt_conflag_uname',\n",
    "             'mqtt_conflag_willflag','mqtt_conflags','mqtt_dupflag','mqtt_hdrflags','mqtt_kalive',\n",
    "             'mqtt_len','mqtt_msg','mqtt_msgid','mqtt_msgtype','mqtt_proto_len','mqtt_protoname',\n",
    "             'mqtt_qos','mqtt_retain','mqtt_sub_qos','mqtt_suback_qos','mqtt_ver','mqtt_willmsg',\n",
    "             'mqtt_willmsg_len','mqtt_willtopic','mqtt_willtopic_len','target','data_category']\n",
    "\n",
    "df_read_underscore = df_read.toDF(*col_names_underscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all zero columns & absurd cols\n",
    "\n",
    "zero_cols = ['mqtt_conack_flags','mqtt_conack_flags_reserved','mqtt_conack_flags_sp','mqtt_conack_val',\n",
    "            'mqtt_conflag_qos','mqtt_conflag_reserved','mqtt_conflag_retain','mqtt_conflag_willflag',\n",
    "            'mqtt_retain','mqtt_sub_qos','mqtt_suback_qos','mqtt_willmsg',\n",
    "             'mqtt_willmsg_len','mqtt_willtopic','mqtt_willtopic_len']\n",
    "\n",
    "absurd_cols = ['mqtt_msg', 'mqtt_msgid']\n",
    "\n",
    "\n",
    "df_read_underscore_dropped = df_read_underscore.drop(*zero_cols+absurd_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test\n",
    "df_read_train = df_read_underscore_dropped.where(df_read_underscore_dropped['data_category'] == 'train')\n",
    "df_read_test = df_read_underscore_dropped.where(df_read_underscore_dropped['data_category'] == 'test')\n",
    "\n",
    "\n",
    "# drop 'data_category' col\n",
    "df_read_train = df_read_train.drop('data_category')\n",
    "df_read_test = df_read_test.drop('data_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        tcp_time_delta   tcp_len  mqtt_conflag_cleansess  \\\n",
      "tcp_time_delta                1.000000 -0.006952               -0.009565   \n",
      "tcp_len                      -0.006952  1.000000               -0.013370   \n",
      "mqtt_conflag_cleansess       -0.009565 -0.013370                1.000000   \n",
      "mqtt_conflag_passwd          -0.006303 -0.008609                0.658921   \n",
      "mqtt_conflag_uname           -0.006312 -0.008623                0.659891   \n",
      "mqtt_dupflag                 -0.018682  0.159388               -0.024359   \n",
      "mqtt_kalive                  -0.005286 -0.008729                0.552715   \n",
      "mqtt_len                     -0.036825  0.274375                0.001855   \n",
      "mqtt_msgtype                  0.283643  0.085299               -0.056337   \n",
      "mqtt_proto_len               -0.009565 -0.013370                1.000000   \n",
      "mqtt_qos                     -0.037996  0.271391               -0.044270   \n",
      "mqtt_ver                     -0.009565 -0.013370                1.000000   \n",
      "\n",
      "                        mqtt_conflag_passwd  mqtt_conflag_uname  mqtt_dupflag  \\\n",
      "tcp_time_delta                    -0.006303           -0.006312     -0.018682   \n",
      "tcp_len                           -0.008609           -0.008623      0.159388   \n",
      "mqtt_conflag_cleansess             0.658921            0.659891     -0.024359   \n",
      "mqtt_conflag_passwd                1.000000            0.998531     -0.016051   \n",
      "mqtt_conflag_uname                 0.998531            1.000000     -0.016074   \n",
      "mqtt_dupflag                      -0.016051           -0.016074      1.000000   \n",
      "mqtt_kalive                       -0.002539           -0.002543     -0.013464   \n",
      "mqtt_len                          -0.005232           -0.005254      0.544121   \n",
      "mqtt_msgtype                      -0.037122           -0.037177      0.125517   \n",
      "mqtt_proto_len                     0.658921            0.659891     -0.024359   \n",
      "mqtt_qos                          -0.029170           -0.029213      0.550238   \n",
      "mqtt_ver                           0.658921            0.659891     -0.024359   \n",
      "\n",
      "                        mqtt_kalive  mqtt_len  mqtt_msgtype  mqtt_proto_len  \\\n",
      "tcp_time_delta            -0.005286 -0.036825      0.283643       -0.009565   \n",
      "tcp_len                   -0.008729  0.274375      0.085299       -0.013370   \n",
      "mqtt_conflag_cleansess     0.552715  0.001855     -0.056337        1.000000   \n",
      "mqtt_conflag_passwd       -0.002539 -0.005232     -0.037122        0.658921   \n",
      "mqtt_conflag_uname        -0.002543 -0.005254     -0.037177        0.659891   \n",
      "mqtt_dupflag              -0.013464  0.544121      0.125517       -0.024359   \n",
      "mqtt_kalive                1.000000 -0.001461     -0.031139        0.552715   \n",
      "mqtt_len                  -0.001461  1.000000      0.261283        0.001855   \n",
      "mqtt_msgtype              -0.031139  0.261283      1.000000       -0.056337   \n",
      "mqtt_proto_len             0.552715  0.001855     -0.056337        1.000000   \n",
      "mqtt_qos                  -0.024469  0.988170      0.228114       -0.044270   \n",
      "mqtt_ver                   0.552715  0.001855     -0.056337        1.000000   \n",
      "\n",
      "                        mqtt_qos  mqtt_ver  \n",
      "tcp_time_delta         -0.037996 -0.009565  \n",
      "tcp_len                 0.271391 -0.013370  \n",
      "mqtt_conflag_cleansess -0.044270  1.000000  \n",
      "mqtt_conflag_passwd    -0.029170  0.658921  \n",
      "mqtt_conflag_uname     -0.029213  0.659891  \n",
      "mqtt_dupflag            0.550238 -0.024359  \n",
      "mqtt_kalive            -0.024469  0.552715  \n",
      "mqtt_len                0.988170  0.001855  \n",
      "mqtt_msgtype            0.228114 -0.056337  \n",
      "mqtt_proto_len         -0.044270  1.000000  \n",
      "mqtt_qos                1.000000 -0.044270  \n",
      "mqtt_ver               -0.044270  1.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8105/2440958697.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  correlation_matrix = df_read_underscore_dropped.toPandas().corr()\n"
     ]
    }
   ],
   "source": [
    "# deal with correlations\n",
    "correlation_matrix = df_read_underscore_dropped.toPandas().corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_underscore = ['tcp_flags','tcp_time_delta','tcp_len','mqtt_conack_flags','mqtt_conack_flags_reserved',\n",
    "             'mqtt_conack_flags_sp','mqtt_conack_val','mqtt_conflag_cleansess','mqtt_conflag_passwd',\n",
    "             'mqtt_conflag_qos','mqtt_conflag_reserved','mqtt_conflag_retain','mqtt_conflag_uname',\n",
    "             'mqtt_conflag_willflag','mqtt_conflags','mqtt_dupflag','mqtt_hdrflags','mqtt_kalive',\n",
    "             'mqtt_len','mqtt_msg','mqtt_msgid','mqtt_msgtype','mqtt_proto_len','mqtt_protoname',\n",
    "             'mqtt_qos','mqtt_retain','mqtt_sub_qos','mqtt_suback_qos','mqtt_ver','mqtt_willmsg',\n",
    "             'mqtt_willmsg_len','mqtt_willtopic','mqtt_willtopic_len','target'] # 'data_category' removed\n",
    "\n",
    "\n",
    "nominal_cols = ['tcp_flags','mqtt_conflags','mqtt_hdrflags', 'mqtt_protoname']\n",
    "\n",
    "continuous_cols = ['tcp_time_delta', 'tcp_len','mqtt_kalive','mqtt_len',  'mqtt_msgtype',\n",
    "                  'mqtt_proto_len','mqtt_qos', 'mqtt_ver']\n",
    "\n",
    "binary_cols = ['mqtt_conflag_cleansess', 'mqtt_conflag_passwd','mqtt_conflag_uname', 'mqtt_dupflag']\n",
    "\n",
    "\n",
    "keys = ['slowite', 'bruteforce','flood', 'malformed', 'dos', 'legitimate']\n",
    "vals = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "global label_dict\n",
    "label_dict = dict(zip(keys, vals))\n",
    "\n",
    "# ===========================================================================\n",
    "\n",
    "class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        label_to_multiclass = udf(lambda name: label_dict[name])\n",
    "        output_df = dataset.withColumn('outcome', label_to_multiclass(col('target'))).drop('target')\n",
    "        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "        return output_df\n",
    "\n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in binary_cols + continuous_cols:\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
    "\n",
    "        return output_df\n",
    "    \n",
    "class ColumnDropper(Transformer): # this transformer drops uannecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "def get_preprocess_pipeline():\n",
    "    # Stage where columns are casted as appropriate types\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n",
    "    \n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "    \n",
    "    # Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "    feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n",
    "    corelated_cols_to_remove = ['mqtt_conflag_uname','mqtt_qos','mqtt_proto_len', 'mqtt_ver']\n",
    "\n",
    "    for col_name in corelated_cols_to_remove:\n",
    "        feature_cols.remove(col_name)\n",
    "    \n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "    \n",
    "\n",
    "    # Stage for creating the outcome column representing whether there is attack \n",
    "    stage_outcome = OutcomeCreater()\n",
    "\n",
    "    # Removing all unnecessary columns, only keeping the 'features' and 'outcome' columns\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols+nominal_id_cols+\n",
    "        nominal_onehot_cols+ binary_cols + continuous_cols + ['vectorized_features'])\n",
    "     \n",
    "        \n",
    "    # fit with logistic regression\n",
    "    lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome', maxIter=10)\n",
    "    \n",
    "    # Connect the columns into a pipeline\n",
    "    pipeline = Pipeline(stages=[stage_typecaster,stage_nominal_indexer,stage_nominal_onehot_encoder,\n",
    "        stage_vector_assembler,stage_scaler,stage_outcome,stage_column_dropper])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform\n",
    "preprocess_pipeline = get_preprocess_pipeline()\n",
    "preprocess_pipeline_model = preprocess_pipeline.fit(df_read_train)\n",
    "\n",
    "train_transform = preprocess_pipeline_model.transform(df_read_train)\n",
    "test_transform = preprocess_pipeline_model.transform(df_read_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning - PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers chosen are: Logistic Regression & Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome', maxIter=5)\n",
    "lr_fit = lr.fit(train_transform)\n",
    "\n",
    "lr_preds_train = lr_fit.transform(train_transform)\n",
    "lr_preds_test = lr_fit.transform(test_transform)\n",
    "\n",
    "# random forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'outcome')\n",
    "rf_fit = rf.fit(train_transform)\n",
    "\n",
    "rf_preds_train = rf_fit.transform(train_transform)\n",
    "rf_preds_test = rf_fit.transform(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Train Accuracy : 0.8225481985443306\n",
      "Test Accuracy : 0.8164692771869925\n",
      "\n",
      "Random Forest\n",
      "Train Accuracy : 0.827085293939891\n",
      "Test Accuracy : 0.8378561012910339\n"
     ]
    }
   ],
   "source": [
    "# accuracies\n",
    "\n",
    "# logistic regression\n",
    "lr_accuracy_train = (lr_preds_train.filter(lr_preds_train.outcome == lr_preds_train.prediction)\n",
    "    .count() / float(lr_preds_train.count()))\n",
    "\n",
    "lr_accuracy_test = (lr_preds_test.filter(lr_preds_test.outcome == lr_preds_test.prediction)\n",
    "    .count() / float(lr_preds_test.count()))\n",
    "\n",
    "print('Logistic Regression')\n",
    "print(\"Train Accuracy :\", lr_accuracy_train)\n",
    "print(\"Test Accuracy :\", lr_accuracy_test)\n",
    "\n",
    "\n",
    "# rf\n",
    "rf_accuracy_train = (rf_preds_train.filter(rf_preds_train.outcome == rf_preds_train.prediction)\n",
    "    .count() / float(rf_preds_train.count()))\n",
    "\n",
    "rf_accuracy_test = (rf_preds_test.filter(rf_preds_test.outcome == rf_preds_test.prediction)\n",
    "    .count() / float(rf_preds_test.count()))\n",
    "\n",
    "print('\\nRandom Forest')\n",
    "print(\"Train Accuracy :\", rf_accuracy_train)\n",
    "print(\"Test Accuracy :\", rf_accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome')\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "lr_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.0001, 1.0])\n",
    "             .addGrid(lr.maxIter, [10, 50])\n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', \n",
    "    labelCol='outcome', metricName='accuracy')\n",
    "\n",
    "lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=lr_paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "lr_cv_fit_train = lr_cv.fit(train_transform)\n",
    "lr_cv_preds_test = lr_cv_fit_train.transform(test_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'outcome')\n",
    "\n",
    "rf_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [10, 15])# maximum depth for each tree\n",
    "             .addGrid(rf.numTrees,[30, 60])# number of trues\n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', \n",
    "    labelCol='outcome', metricName='accuracy')\n",
    "\n",
    "rf_cv = CrossValidator(estimator=rf, estimatorParamMaps=rf_paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "rf_cv_fit_train = rf_cv.fit(train_transform)\n",
    "\n",
    "rf_cv_preds_test = rf_cv_fit_train.transform(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression\n",
      "Pre-CV: 0.8164692771869925\n",
      "Post-CV: 0.8269137295349196\n",
      "\n",
      "Random Forest\n",
      "Pre-CV: 0.8378561012910339\n",
      "Post-CV: 0.9015660750909888\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "lr_accuracy_test_cv = (lr_cv_preds_test.filter(lr_cv_preds_test.outcome == lr_cv_preds_test.prediction)\n",
    "    .count() / float(lr_cv_preds_test.count()))\n",
    "\n",
    "# random forest\n",
    "rf_accuracy_test_cv = (rf_cv_preds_test.filter(rf_cv_preds_test.outcome == rf_cv_preds_test.prediction)\n",
    "    .count() / float(rf_cv_preds_test.count()))\n",
    "\n",
    "print('\\nLogistic Regression')\n",
    "print(\"Pre-CV:\", lr_accuracy_test)\n",
    "print(\"Post-CV:\", lr_accuracy_test_cv)\n",
    "\n",
    "\n",
    "print(\"\\nRandom Forest\")\n",
    "print(\"Pre-CV:\", rf_accuracy_test)\n",
    "print(\"Post-CV:\", rf_accuracy_test_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning - TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two classifiers chosen are: a shallow NN and a deep NN. The shallow NN only has 2 hidden layers, while the deep NN has 5 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # now import the tensorflow module\n",
    "print(tf.__version__)  # make sure the version is 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create tensors\n",
    "\n",
    "x_train = tf.constant(np.array(train_transform.toPandas()['features'].values.tolist()))\n",
    "y_train = tf.constant(np.array(train_transform.toPandas()['outcome'].values.tolist()))\n",
    "\n",
    "x_test = tf.constant(np.array(test_transform.toPandas()['features'].values.tolist()))\n",
    "y_test = tf.constant(np.array(test_transform.toPandas()['outcome'].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7239/7239 - 4s - loss: 0.4974 - sparse_categorical_accuracy: 0.7947 - 4s/epoch - 526us/step\n",
      "Epoch 2/10\n",
      "7239/7239 - 4s - loss: 0.4424 - sparse_categorical_accuracy: 0.8144 - 4s/epoch - 524us/step\n",
      "Epoch 3/10\n",
      "7239/7239 - 4s - loss: 0.4354 - sparse_categorical_accuracy: 0.8186 - 4s/epoch - 504us/step\n",
      "Epoch 4/10\n",
      "7239/7239 - 4s - loss: 0.4307 - sparse_categorical_accuracy: 0.8219 - 4s/epoch - 503us/step\n",
      "Epoch 5/10\n",
      "7239/7239 - 4s - loss: 0.4273 - sparse_categorical_accuracy: 0.8237 - 4s/epoch - 575us/step\n",
      "Epoch 6/10\n",
      "7239/7239 - 4s - loss: 0.4240 - sparse_categorical_accuracy: 0.8257 - 4s/epoch - 549us/step\n",
      "Epoch 7/10\n",
      "7239/7239 - 4s - loss: 0.4226 - sparse_categorical_accuracy: 0.8258 - 4s/epoch - 515us/step\n",
      "Epoch 8/10\n",
      "7239/7239 - 4s - loss: 0.4218 - sparse_categorical_accuracy: 0.8260 - 4s/epoch - 504us/step\n",
      "Epoch 9/10\n",
      "7239/7239 - 4s - loss: 0.4213 - sparse_categorical_accuracy: 0.8261 - 4s/epoch - 504us/step\n",
      "Epoch 10/10\n",
      "7239/7239 - 4s - loss: 0.4210 - sparse_categorical_accuracy: 0.8259 - 4s/epoch - 507us/step\n",
      "2636/2636 - 1s - loss: 0.4314 - sparse_categorical_accuracy: 0.8270 - 1s/epoch - 517us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4313971996307373, 0.8269611597061157]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shallow NN (2 hidden layers)\n",
    "from tensorflow import keras\n",
    "\n",
    "model_shallow = keras.Sequential()\n",
    "\n",
    "model_shallow.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "model_shallow.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "\n",
    "model_shallow.add(keras.layers.Dense(6))\n",
    "\n",
    "model_shallow.compile(optimizer = keras.optimizers.SGD(), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model_shallow.fit(x_train, y_train, epochs = 10, verbose = 2)\n",
    "model_shallow.evaluate(x_test, y_test, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7239/7239 - 4s - loss: 0.5754 - sparse_categorical_accuracy: 0.7747 - 4s/epoch - 612us/step\n",
      "Epoch 2/10\n",
      "7239/7239 - 4s - loss: 0.4409 - sparse_categorical_accuracy: 0.8191 - 4s/epoch - 541us/step\n",
      "Epoch 3/10\n",
      "7239/7239 - 4s - loss: 0.4365 - sparse_categorical_accuracy: 0.8215 - 4s/epoch - 529us/step\n",
      "Epoch 4/10\n",
      "7239/7239 - 4s - loss: 0.4362 - sparse_categorical_accuracy: 0.8216 - 4s/epoch - 531us/step\n",
      "Epoch 5/10\n",
      "7239/7239 - 5s - loss: 0.4340 - sparse_categorical_accuracy: 0.8218 - 5s/epoch - 640us/step\n",
      "Epoch 6/10\n",
      "7239/7239 - 4s - loss: 0.4335 - sparse_categorical_accuracy: 0.8219 - 4s/epoch - 603us/step\n",
      "Epoch 7/10\n",
      "7239/7239 - 4s - loss: 0.4328 - sparse_categorical_accuracy: 0.8218 - 4s/epoch - 574us/step\n",
      "Epoch 8/10\n",
      "7239/7239 - 4s - loss: 0.4320 - sparse_categorical_accuracy: 0.8218 - 4s/epoch - 538us/step\n",
      "Epoch 9/10\n",
      "7239/7239 - 4s - loss: 0.4314 - sparse_categorical_accuracy: 0.8217 - 4s/epoch - 537us/step\n",
      "Epoch 10/10\n",
      "7239/7239 - 4s - loss: 0.4303 - sparse_categorical_accuracy: 0.8219 - 4s/epoch - 532us/step\n",
      "2636/2636 - 1s - loss: 0.4407 - sparse_categorical_accuracy: 0.8159 - 1s/epoch - 516us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44072425365448, 0.8159002065658569]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deep NN (2 hidden layers)\n",
    "from tensorflow import keras\n",
    "\n",
    "model_deep = keras.Sequential()\n",
    "\n",
    "model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n",
    "\n",
    "\n",
    "model_deep.add(keras.layers.Dense(6))\n",
    "\n",
    "model_deep.compile(optimizer = keras.optimizers.SGD(), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model_deep.fit(x_train, y_train, epochs = 10, verbose = 2)\n",
    "model_deep.evaluate(x_test, y_test, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the x and y train tensors and shuffle\n",
    "tf_train = tf.concat([x_train, tf.reshape(y_train, [-1, 1])], axis = 1)\n",
    "tf_train_shuffle = tf.random.shuffle(tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_cross_val_activation_width(k, act_fun, width):\n",
    "    \n",
    "    cuts = np.linspace(0, tf_train_shuffle.shape[0]-1, k+1, dtype = int)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    model.add(keras.layers.Dense(6))\n",
    "\n",
    "    metric = []\n",
    "\n",
    "    for i in range(k):\n",
    "        print('k = '+str(i+1)+'\\n')\n",
    "        \n",
    "        val = tf_train_shuffle[cuts[i]:cuts[i+1]]\n",
    "            \n",
    "        mask = np.ones(tf_train_shuffle.shape[0])\n",
    "        mask[cuts[i]:cuts[i+1]] = 0\n",
    "        \n",
    "        train = tf.boolean_mask(tf_train_shuffle, mask)\n",
    "    \n",
    "        cur_x_train = train[:,:-1]\n",
    "        cur_y_train = train[:,-1]\n",
    "        \n",
    "        cur_x_val = val[:,:-1]\n",
    "        cur_y_val = val[:,-1]\n",
    "        \n",
    "        model.compile(optimizer = keras.optimizers.SGD(), \n",
    "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "        \n",
    "        fit_data = model.fit(cur_x_train, cur_y_train, epochs = 5, \n",
    "                             verbose = 2,validation_data = (cur_x_val, cur_y_val))\n",
    "        \n",
    "        cur_auc = np.mean(fit_data.history['val_sparse_categorical_accuracy'])\n",
    "        metric.append(cur_auc)\n",
    "        \n",
    "        if np.max(metric) == cur_auc:\n",
    "            best_model = model\n",
    "            print('\\nNew best model saved.')\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "    print(metric)\n",
    "    return np.mean(metric), best_model.evaluate(x_test, y_test, verbose = 2), best_model\n",
    "\n",
    "\n",
    "# val_acc, test_res, best_model_shallow = shallow_cross_val_activation_width(k = 4, act_fun = 'relu', width = 5)\n",
    "\n",
    "# print('\\n===========')\n",
    "# print(\"Validation Accuracy:\", val_acc)\n",
    "# print(\"Test Accuracy:\", test_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_cross_val_activation_width(k, act_fun, width):\n",
    "    \n",
    "    cuts = np.linspace(0, tf_train_shuffle.shape[0]-1, k+1, dtype = int)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    model.add(keras.layers.Dense(width, activation = act_fun))\n",
    "    \n",
    "    model.add(keras.layers.Dense(6))\n",
    "\n",
    "    metric = []\n",
    "\n",
    "    for i in range(k):\n",
    "        print('k = '+str(i+1)+'\\n')\n",
    "        \n",
    "        val = tf_train_shuffle[cuts[i]:cuts[i+1]]\n",
    "            \n",
    "        mask = np.ones(tf_train_shuffle.shape[0])\n",
    "        mask[cuts[i]:cuts[i+1]] = 0\n",
    "        \n",
    "        train = tf.boolean_mask(tf_train_shuffle, mask)\n",
    "    \n",
    "        cur_x_train = train[:,:-1]\n",
    "        cur_y_train = train[:,-1]\n",
    "        \n",
    "        cur_x_val = val[:,:-1]\n",
    "        cur_y_val = val[:,-1]\n",
    "        \n",
    "        model.compile(optimizer = keras.optimizers.SGD(), \n",
    "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "        \n",
    "        fit_data = model.fit(cur_x_train, cur_y_train, epochs = 5,\n",
    "                             verbose = 2, validation_data = (cur_x_val, cur_y_val))\n",
    "        \n",
    "        cur_auc = np.mean(fit_data.history['val_sparse_categorical_accuracy'])\n",
    "        metric.append(cur_auc)\n",
    "        \n",
    "                \n",
    "        if np.max(metric) == cur_auc:\n",
    "            best_model = model\n",
    "            print('\\nNew best model saved.')\n",
    "\n",
    "        print('\\n')\n",
    "        \n",
    "    print(metric)\n",
    "    return np.mean(metric), best_model.evaluate(x_train, y_train, verbose = 2), best_model\n",
    "\n",
    "\n",
    "# val_acc, test_res, best_model_deep = deep_cross_val_activation_width(k = 4, act_fun = 'relu', width = 5)\n",
    "\n",
    "# print('\\n===========')\n",
    "# print(\"Validation Accuracy:\", val_acc)\n",
    "# print(\"Test Accuracy:\", test_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting trial: run-relu width10\n",
      "k = 1\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.5097 - sparse_categorical_accuracy: 0.7935 - val_loss: 0.4405 - val_sparse_categorical_accuracy: 0.8218 - 4s/epoch - 819us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.4387 - sparse_categorical_accuracy: 0.8146 - val_loss: 0.4323 - val_sparse_categorical_accuracy: 0.8241 - 4s/epoch - 756us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.4315 - sparse_categorical_accuracy: 0.8197 - val_loss: 0.4290 - val_sparse_categorical_accuracy: 0.8241 - 4s/epoch - 769us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.4260 - sparse_categorical_accuracy: 0.8236 - val_loss: 0.4316 - val_sparse_categorical_accuracy: 0.8241 - 4s/epoch - 858us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.4241 - sparse_categorical_accuracy: 0.8247 - val_loss: 0.4362 - val_sparse_categorical_accuracy: 0.7520 - 4s/epoch - 774us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.4231 - sparse_categorical_accuracy: 0.8248 - val_loss: 0.4234 - val_sparse_categorical_accuracy: 0.8240 - 4s/epoch - 814us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.4225 - sparse_categorical_accuracy: 0.8255 - val_loss: 0.4233 - val_sparse_categorical_accuracy: 0.8257 - 4s/epoch - 750us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.4218 - sparse_categorical_accuracy: 0.8254 - val_loss: 0.4262 - val_sparse_categorical_accuracy: 0.8238 - 4s/epoch - 749us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.4214 - sparse_categorical_accuracy: 0.8259 - val_loss: 0.4228 - val_sparse_categorical_accuracy: 0.8255 - 4s/epoch - 765us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.4211 - sparse_categorical_accuracy: 0.8257 - val_loss: 0.4237 - val_sparse_categorical_accuracy: 0.8259 - 4s/epoch - 800us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.4237 - sparse_categorical_accuracy: 0.8248 - val_loss: 0.4201 - val_sparse_categorical_accuracy: 0.8273 - 4s/epoch - 800us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.4234 - sparse_categorical_accuracy: 0.8248 - val_loss: 0.4213 - val_sparse_categorical_accuracy: 0.8254 - 4s/epoch - 755us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.4234 - sparse_categorical_accuracy: 0.8246 - val_loss: 0.4233 - val_sparse_categorical_accuracy: 0.8273 - 4s/epoch - 790us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.4229 - sparse_categorical_accuracy: 0.8251 - val_loss: 0.4172 - val_sparse_categorical_accuracy: 0.8283 - 4s/epoch - 757us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.4229 - sparse_categorical_accuracy: 0.8247 - val_loss: 0.4223 - val_sparse_categorical_accuracy: 0.8283 - 4s/epoch - 762us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "[0.8092339634895325, 0.8249951481819153, 0.8273003935813904]\n",
      "2636/2636 - 1s - loss: 0.4344 - sparse_categorical_accuracy: 0.8270 - 1s/epoch - 541us/step\n",
      "New Best Model: run-relu width10\n",
      "\n",
      "--- Starting trial: run-relu width20\n",
      "k = 1\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.5043 - sparse_categorical_accuracy: 0.7975 - val_loss: 0.4375 - val_sparse_categorical_accuracy: 0.8211 - 4s/epoch - 846us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.4382 - sparse_categorical_accuracy: 0.8163 - val_loss: 0.4525 - val_sparse_categorical_accuracy: 0.7496 - 4s/epoch - 761us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.4306 - sparse_categorical_accuracy: 0.8200 - val_loss: 0.4433 - val_sparse_categorical_accuracy: 0.8219 - 4s/epoch - 775us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.4270 - sparse_categorical_accuracy: 0.8230 - val_loss: 0.4247 - val_sparse_categorical_accuracy: 0.8245 - 4s/epoch - 808us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.4248 - sparse_categorical_accuracy: 0.8255 - val_loss: 0.4240 - val_sparse_categorical_accuracy: 0.8247 - 4s/epoch - 823us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.4237 - sparse_categorical_accuracy: 0.8256 - val_loss: 0.4231 - val_sparse_categorical_accuracy: 0.8259 - 4s/epoch - 793us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.4225 - sparse_categorical_accuracy: 0.8256 - val_loss: 0.4274 - val_sparse_categorical_accuracy: 0.8259 - 4s/epoch - 791us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.4215 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.4228 - val_sparse_categorical_accuracy: 0.8259 - 4s/epoch - 757us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.4208 - sparse_categorical_accuracy: 0.8259 - val_loss: 0.4226 - val_sparse_categorical_accuracy: 0.8259 - 4s/epoch - 806us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.4206 - sparse_categorical_accuracy: 0.8261 - val_loss: 0.4215 - val_sparse_categorical_accuracy: 0.8257 - 4s/epoch - 784us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.4229 - sparse_categorical_accuracy: 0.8249 - val_loss: 0.4162 - val_sparse_categorical_accuracy: 0.8282 - 4s/epoch - 811us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.4228 - sparse_categorical_accuracy: 0.8251 - val_loss: 0.4167 - val_sparse_categorical_accuracy: 0.8282 - 4s/epoch - 760us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.4225 - sparse_categorical_accuracy: 0.8250 - val_loss: 0.4160 - val_sparse_categorical_accuracy: 0.8283 - 4s/epoch - 777us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.4223 - sparse_categorical_accuracy: 0.8247 - val_loss: 0.4171 - val_sparse_categorical_accuracy: 0.8283 - 4s/epoch - 761us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.4222 - sparse_categorical_accuracy: 0.8252 - val_loss: 0.4171 - val_sparse_categorical_accuracy: 0.8273 - 4s/epoch - 761us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "[0.8083610653877258, 0.8258550882339477, 0.8280644893646241]\n",
      "2636/2636 - 1s - loss: 0.4332 - sparse_categorical_accuracy: 0.8238 - 1s/epoch - 500us/step\n",
      "\n",
      "--- Starting trial: run-softmax width10\n",
      "k = 1\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 1.1332 - sparse_categorical_accuracy: 0.4965 - val_loss: 1.0748 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 826us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 1.0610 - sparse_categorical_accuracy: 0.5089 - val_loss: 1.0341 - val_sparse_categorical_accuracy: 0.6159 - 4s/epoch - 786us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.8645 - sparse_categorical_accuracy: 0.7391 - val_loss: 0.6878 - val_sparse_categorical_accuracy: 0.7820 - 4s/epoch - 770us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.6631 - sparse_categorical_accuracy: 0.7741 - val_loss: 0.6239 - val_sparse_categorical_accuracy: 0.7820 - 4s/epoch - 760us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.6096 - sparse_categorical_accuracy: 0.7822 - val_loss: 0.5956 - val_sparse_categorical_accuracy: 0.7820 - 4s/epoch - 783us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.5852 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.5809 - val_sparse_categorical_accuracy: 0.7822 - 4s/epoch - 897us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.5760 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.5716 - val_sparse_categorical_accuracy: 0.7822 - 4s/epoch - 791us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.5643 - sparse_categorical_accuracy: 0.7827 - val_loss: 0.5548 - val_sparse_categorical_accuracy: 0.7822 - 4s/epoch - 792us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.5383 - sparse_categorical_accuracy: 0.7829 - val_loss: 0.5256 - val_sparse_categorical_accuracy: 0.7822 - 4s/epoch - 796us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.5094 - sparse_categorical_accuracy: 0.7973 - val_loss: 0.5014 - val_sparse_categorical_accuracy: 0.8051 - 4s/epoch - 825us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.4921 - sparse_categorical_accuracy: 0.8047 - val_loss: 0.4826 - val_sparse_categorical_accuracy: 0.8069 - 4s/epoch - 852us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.4817 - sparse_categorical_accuracy: 0.8054 - val_loss: 0.4746 - val_sparse_categorical_accuracy: 0.8115 - 4s/epoch - 764us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.4756 - sparse_categorical_accuracy: 0.8089 - val_loss: 0.4704 - val_sparse_categorical_accuracy: 0.8115 - 4s/epoch - 806us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.4706 - sparse_categorical_accuracy: 0.8089 - val_loss: 0.4646 - val_sparse_categorical_accuracy: 0.8115 - 4s/epoch - 785us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.4663 - sparse_categorical_accuracy: 0.8089 - val_loss: 0.4614 - val_sparse_categorical_accuracy: 0.8115 - 4s/epoch - 771us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "[0.6922204375267029, 0.7867512702941895, 0.8105445861816406]\n",
      "2636/2636 - 1s - loss: 0.4701 - sparse_categorical_accuracy: 0.8074 - 1s/epoch - 502us/step\n",
      "\n",
      "--- Starting trial: run-softmax width20\n",
      "k = 1\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 1.1335 - sparse_categorical_accuracy: 0.5003 - val_loss: 1.0814 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 835us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 1.0787 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0764 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 775us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 1.0750 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0721 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 791us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 1.0550 - sparse_categorical_accuracy: 0.5050 - val_loss: 1.0145 - val_sparse_categorical_accuracy: 0.6559 - 4s/epoch - 779us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.8440 - sparse_categorical_accuracy: 0.7772 - val_loss: 0.7150 - val_sparse_categorical_accuracy: 0.7817 - 4s/epoch - 863us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.6771 - sparse_categorical_accuracy: 0.7796 - val_loss: 0.6538 - val_sparse_categorical_accuracy: 0.7819 - 4s/epoch - 886us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.6348 - sparse_categorical_accuracy: 0.7809 - val_loss: 0.6240 - val_sparse_categorical_accuracy: 0.7819 - 4s/epoch - 792us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.6104 - sparse_categorical_accuracy: 0.7825 - val_loss: 0.6052 - val_sparse_categorical_accuracy: 0.7819 - 4s/epoch - 767us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.5953 - sparse_categorical_accuracy: 0.7826 - val_loss: 0.5925 - val_sparse_categorical_accuracy: 0.7819 - 4s/epoch - 838us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.5851 - sparse_categorical_accuracy: 0.7826 - val_loss: 0.5843 - val_sparse_categorical_accuracy: 0.7819 - 4s/epoch - 763us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.5802 - sparse_categorical_accuracy: 0.7818 - val_loss: 0.5753 - val_sparse_categorical_accuracy: 0.7835 - 4s/epoch - 843us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.5752 - sparse_categorical_accuracy: 0.7818 - val_loss: 0.5711 - val_sparse_categorical_accuracy: 0.7835 - 4s/epoch - 770us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.5712 - sparse_categorical_accuracy: 0.7818 - val_loss: 0.5673 - val_sparse_categorical_accuracy: 0.7835 - 4s/epoch - 833us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.5679 - sparse_categorical_accuracy: 0.7818 - val_loss: 0.5641 - val_sparse_categorical_accuracy: 0.7835 - 4s/epoch - 818us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.5648 - sparse_categorical_accuracy: 0.7818 - val_loss: 0.5610 - val_sparse_categorical_accuracy: 0.7835 - 4s/epoch - 894us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "[0.5869662642478943, 0.7819361686706543, 0.7834746837615967]\n",
      "2636/2636 - 1s - loss: 0.5659 - sparse_categorical_accuracy: 0.7810 - 1s/epoch - 558us/step\n",
      "2636/2636 - 2s - loss: 0.4344 - sparse_categorical_accuracy: 0.8270 - 2s/epoch - 614us/step\n",
      "Best Model Hyperparameters: run-relu width10\n",
      "Test Accuracy: 0.8269611597061157\n"
     ]
    }
   ],
   "source": [
    "act_funs = ['relu', 'softmax']\n",
    "widths = [10, 20]\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for act_fun in act_funs:\n",
    "    for width in widths:\n",
    "\n",
    "        run_name = 'run-'+act_fun+' width'+str(width)\n",
    "        print('')\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "\n",
    "        run_dir = 'logs14813/hparam_tuning_q2_2/' + run_name\n",
    "        val_acc, test_res, hp_model = shallow_cross_val_activation_width(3, act_fun, width)\n",
    "        \n",
    "        accuracies.append(test_res[1])\n",
    "        \n",
    "        if np.max(accuracies) == test_res[1]:\n",
    "            best_hp_shallow_model = hp_model\n",
    "            best_shallow_run_name = run_name\n",
    "            print('New Best Model:', best_shallow_run_name)\n",
    "            \n",
    "shallow_res = best_hp_shallow_model.evaluate(x_test, y_test, verbose = 2)\n",
    "\n",
    "print('Best Model Hyperparameters:', best_shallow_run_name)\n",
    "print('Test Accuracy:', shallow_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting trial: run-relu width10\n",
      "k = 1\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.5223 - sparse_categorical_accuracy: 0.7980 - val_loss: 0.4646 - val_sparse_categorical_accuracy: 0.8185 - 4s/epoch - 903us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.4520 - sparse_categorical_accuracy: 0.8161 - val_loss: 0.4432 - val_sparse_categorical_accuracy: 0.8183 - 4s/epoch - 808us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.4431 - sparse_categorical_accuracy: 0.8170 - val_loss: 0.4423 - val_sparse_categorical_accuracy: 0.8202 - 4s/epoch - 791us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.4364 - sparse_categorical_accuracy: 0.8218 - val_loss: 0.4359 - val_sparse_categorical_accuracy: 0.8206 - 4s/epoch - 808us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.4316 - sparse_categorical_accuracy: 0.8236 - val_loss: 0.4350 - val_sparse_categorical_accuracy: 0.8239 - 4s/epoch - 786us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 5s - loss: 0.4281 - sparse_categorical_accuracy: 0.8242 - val_loss: 0.4296 - val_sparse_categorical_accuracy: 0.8252 - 5s/epoch - 1ms/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.4257 - sparse_categorical_accuracy: 0.8247 - val_loss: 0.4282 - val_sparse_categorical_accuracy: 0.8252 - 4s/epoch - 815us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.4248 - sparse_categorical_accuracy: 0.8247 - val_loss: 0.4252 - val_sparse_categorical_accuracy: 0.8252 - 4s/epoch - 812us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.4239 - sparse_categorical_accuracy: 0.8247 - val_loss: 0.4262 - val_sparse_categorical_accuracy: 0.8252 - 4s/epoch - 867us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.4235 - sparse_categorical_accuracy: 0.8251 - val_loss: 0.4233 - val_sparse_categorical_accuracy: 0.8252 - 4s/epoch - 808us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.4253 - sparse_categorical_accuracy: 0.8240 - val_loss: 0.4189 - val_sparse_categorical_accuracy: 0.8273 - 4s/epoch - 879us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.4248 - sparse_categorical_accuracy: 0.8242 - val_loss: 0.4194 - val_sparse_categorical_accuracy: 0.8273 - 4s/epoch - 828us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.4247 - sparse_categorical_accuracy: 0.8239 - val_loss: 0.4194 - val_sparse_categorical_accuracy: 0.8257 - 4s/epoch - 906us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.4244 - sparse_categorical_accuracy: 0.8238 - val_loss: 0.4242 - val_sparse_categorical_accuracy: 0.8257 - 4s/epoch - 823us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.4241 - sparse_categorical_accuracy: 0.8240 - val_loss: 0.4223 - val_sparse_categorical_accuracy: 0.8273 - 4s/epoch - 821us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "[0.8202913880348206, 0.8251842260360718, 0.8266295671463013]\n",
      "7239/7239 - 4s - loss: 0.4258 - sparse_categorical_accuracy: 0.8254 - 4s/epoch - 510us/step\n",
      "New Best Model: run-relu width10\n",
      "\n",
      "--- Starting trial: run-relu width20\n",
      "k = 1\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.4952 - sparse_categorical_accuracy: 0.8029 - val_loss: 0.4468 - val_sparse_categorical_accuracy: 0.8219 - 4s/epoch - 887us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.4308 - sparse_categorical_accuracy: 0.8229 - val_loss: 0.4277 - val_sparse_categorical_accuracy: 0.8246 - 4s/epoch - 813us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.4249 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.4253 - val_sparse_categorical_accuracy: 0.8240 - 4s/epoch - 804us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.4223 - sparse_categorical_accuracy: 0.8266 - val_loss: 0.4255 - val_sparse_categorical_accuracy: 0.8247 - 4s/epoch - 812us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.4210 - sparse_categorical_accuracy: 0.8264 - val_loss: 0.4273 - val_sparse_categorical_accuracy: 0.8240 - 4s/epoch - 846us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.4202 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.4231 - val_sparse_categorical_accuracy: 0.8259 - 4s/epoch - 898us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.4197 - sparse_categorical_accuracy: 0.8257 - val_loss: 0.4219 - val_sparse_categorical_accuracy: 0.8259 - 4s/epoch - 891us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 5s - loss: 0.4191 - sparse_categorical_accuracy: 0.8262 - val_loss: 0.4214 - val_sparse_categorical_accuracy: 0.8258 - 5s/epoch - 971us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.4192 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.4195 - val_sparse_categorical_accuracy: 0.8256 - 4s/epoch - 807us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.4189 - sparse_categorical_accuracy: 0.8261 - val_loss: 0.4229 - val_sparse_categorical_accuracy: 0.8240 - 4s/epoch - 820us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 0.4211 - sparse_categorical_accuracy: 0.8250 - val_loss: 0.4144 - val_sparse_categorical_accuracy: 0.8283 - 4s/epoch - 932us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 0.4208 - sparse_categorical_accuracy: 0.8250 - val_loss: 0.4168 - val_sparse_categorical_accuracy: 0.8286 - 4s/epoch - 917us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 0.4206 - sparse_categorical_accuracy: 0.8252 - val_loss: 0.4174 - val_sparse_categorical_accuracy: 0.8251 - 4s/epoch - 875us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 0.4205 - sparse_categorical_accuracy: 0.8251 - val_loss: 0.4142 - val_sparse_categorical_accuracy: 0.8269 - 4s/epoch - 829us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 0.4202 - sparse_categorical_accuracy: 0.8252 - val_loss: 0.4137 - val_sparse_categorical_accuracy: 0.8285 - 4s/epoch - 864us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "[0.8238425254821777, 0.8254535913467407, 0.8274791240692139]\n",
      "7239/7239 - 4s - loss: 0.4176 - sparse_categorical_accuracy: 0.8265 - 4s/epoch - 524us/step\n",
      "New Best Model: run-relu width20\n",
      "\n",
      "--- Starting trial: run-softmax width10\n",
      "k = 1\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 5s - loss: 1.1283 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0817 - val_sparse_categorical_accuracy: 0.4990 - 5s/epoch - 943us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 1.0795 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0777 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 843us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 1.0770 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0763 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 830us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 1.0761 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0755 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 841us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 1.0755 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0750 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 835us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 1.0732 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0790 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 872us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 1.0730 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0789 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 817us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 1.0728 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0788 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 835us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 1.0727 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0787 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 889us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 1.0727 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0786 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 900us/step\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 5s - loss: 1.0765 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0709 - val_sparse_categorical_accuracy: 0.5019 - 5s/epoch - 988us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 1.0765 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0709 - val_sparse_categorical_accuracy: 0.5019 - 4s/epoch - 823us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 1.0764 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0710 - val_sparse_categorical_accuracy: 0.5019 - 4s/epoch - 825us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 1.0764 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0709 - val_sparse_categorical_accuracy: 0.5019 - 4s/epoch - 812us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 1.0764 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0709 - val_sparse_categorical_accuracy: 0.5019 - 4s/epoch - 814us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "[0.499048113822937, 0.4990222156047821, 0.501936137676239]\n",
      "7239/7239 - 4s - loss: 1.0745 - sparse_categorical_accuracy: 0.5000 - 4s/epoch - 527us/step\n",
      "\n",
      "--- Starting trial: run-softmax width20\n",
      "k = 1\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 1.1332 - sparse_categorical_accuracy: 0.4992 - val_loss: 1.0822 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 901us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 1.0798 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0778 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 822us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 1.0772 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0763 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 843us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 1.0762 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0755 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 828us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 1.0756 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0751 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 834us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 1.0732 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0791 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 895us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 1.0730 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0789 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 824us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 1.0729 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0788 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 811us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 1.0728 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0787 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 819us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 1.0727 - sparse_categorical_accuracy: 0.5005 - val_loss: 1.0787 - val_sparse_categorical_accuracy: 0.4990 - 4s/epoch - 821us/step\n",
      "\n",
      "\n",
      "k = 3\n",
      "\n",
      "Epoch 1/5\n",
      "4826/4826 - 4s - loss: 1.0765 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0709 - val_sparse_categorical_accuracy: 0.5019 - 4s/epoch - 890us/step\n",
      "Epoch 2/5\n",
      "4826/4826 - 4s - loss: 1.0765 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0709 - val_sparse_categorical_accuracy: 0.5019 - 4s/epoch - 821us/step\n",
      "Epoch 3/5\n",
      "4826/4826 - 4s - loss: 1.0765 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0709 - val_sparse_categorical_accuracy: 0.5019 - 4s/epoch - 835us/step\n",
      "Epoch 4/5\n",
      "4826/4826 - 4s - loss: 1.0764 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0709 - val_sparse_categorical_accuracy: 0.5019 - 4s/epoch - 871us/step\n",
      "Epoch 5/5\n",
      "4826/4826 - 4s - loss: 1.0764 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.0709 - val_sparse_categorical_accuracy: 0.5019 - 4s/epoch - 832us/step\n",
      "\n",
      "New best model saved.\n",
      "\n",
      "\n",
      "[0.499048113822937, 0.4990222156047821, 0.501936137676239]\n",
      "7239/7239 - 4s - loss: 1.0745 - sparse_categorical_accuracy: 0.5000 - 4s/epoch - 516us/step\n",
      "2636/2636 - 1s - loss: 0.4265 - sparse_categorical_accuracy: 0.8273 - 1s/epoch - 514us/step\n",
      "Best Model Hyperparameters: run-relu width20\n",
      "Test Accuracy: 0.8272812366485596\n"
     ]
    }
   ],
   "source": [
    "act_funs = ['relu', 'softmax']\n",
    "widths = [10, 20]\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for act_fun in act_funs:\n",
    "    for width in widths:\n",
    "\n",
    "        run_name = 'run-'+act_fun+' width'+str(width)\n",
    "        print('')\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        \n",
    "        val_acc, test_res, hp_model = deep_cross_val_activation_width(3, act_fun, width)\n",
    "        \n",
    "        accuracies.append(test_res[1])\n",
    "        \n",
    "        if np.max(accuracies) == test_res[1]:\n",
    "            best_hp_deep_model = hp_model\n",
    "            best_deep_run_name = run_name\n",
    "            print('New Best Model:', best_deep_run_name)\n",
    "\n",
    "\n",
    "deep_res = best_hp_deep_model.evaluate(x_test, y_test, verbose = 2)\n",
    "\n",
    "print('Best Model Hyperparameters:', best_deep_run_name)\n",
    "print('Test Accuracy:', deep_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
