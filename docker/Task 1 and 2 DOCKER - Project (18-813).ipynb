{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce159d2-4662-4588-b747-5d20a6b4e1d4",
   "metadata": {},
   "source": [
    "Variables to change:\n",
    "\n",
    "1. jar_file_loc: this is the file of the .jar postgres file. Change path to where it is located\n",
    "2. train_data_path & test_data_path: change to path of train/test csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f67f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APP Name :GenericAppName\n",
      "Master :local[*]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "jar_file_loc = 'postgresql-42.5.0.jar'\n",
    "\n",
    "# set up Spark\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GenericAppName\") \\\n",
    "    .config('spark.jars', jar_file_loc) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#Access SparkContext from your SparkSession\n",
    "print(\"APP Name :\"+ spark.sparkContext.appName);\n",
    "print(\"Master :\"+ spark.sparkContext.master);\n",
    "\n",
    "sqlContext = SQLContext(spark.sparkContext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c90b7",
   "metadata": {},
   "source": [
    "I tried using the augmented CSVs, but I couldn't succcessfully read it from Postgres after I wrote to it. The read command to more than 10 minutes and still wasn't able to produce a result. So, I decided to use the reduced dataset instead. I will run this on the cloud for the final submissions, so with the higher computation power, I should be able to analyze the augmented files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462a6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df_train = spark.read.csv('data_folder/train70_reduced.csv', header = True, inferSchema = True)\n",
    "df_test = spark.read.csv('data_folder/test30_reduced.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95037873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 231646\n",
      "Test 84351\n",
      "Combined 315997\n"
     ]
    }
   ],
   "source": [
    "# add column to differentiate b/w train and test sets\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "df_train_cat = df_train.withColumn(\"data_category\", lit(\"train\"))\n",
    "df_test_cat = df_test.withColumn(\"data_category\", lit(\"test\"))\n",
    "\n",
    "print('Train', df_train_cat.count())\n",
    "print('Test', df_test_cat.count())\n",
    "\n",
    "\n",
    "# combine dfs\n",
    "df_combined = df_train_cat.union(df_test_cat)\n",
    "print('Combined', df_combined.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b68f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write into postgresql db\n",
    "\n",
    "db_properties={}\n",
    "#update your db username\n",
    "db_properties['username']=\"postgres\"\n",
    "#update your db password\n",
    "db_properties['password']=\"bigdata\"\n",
    "#make sure you got the right port number here\n",
    "db_properties['url']= \"jdbc:postgresql://host.docker.internal/postgres\"\n",
    "#make sure you had the Postgres JAR file in the right location\n",
    "db_properties['driver']=\"org.postgresql.Driver\"\n",
    "db_properties['table']= \"mqtt\"\n",
    "\n",
    "# create df with train data \n",
    "df_combined.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", \"bigdata\")\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85f91954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item Count from PostgreSQL Read: 315997\n"
     ]
    }
   ],
   "source": [
    "# read db to ensure data has been written in correctly\n",
    "df_read = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", \"bigdata\")\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()\n",
    "\n",
    "print('Item Count from PostgreSQL Read:', df_read.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80635bf0-ab9d-4c69-b931-82496b524a5f",
   "metadata": {},
   "source": [
    "# Task II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fdee3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of a MQtt message in the training dataset is: 31.435725201384873\n"
     ]
    }
   ],
   "source": [
    "# average length of a MQTT message in train\n",
    "df_train = df_read.filter(df_read['data_category'] == 'train')\n",
    "\n",
    "print('Average length of a MQtt message in the training dataset is:',\\\n",
    "      df_train.agg({'`mqtt.len`':'mean'}).collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e25028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|    target|      avg(tcp.len)|\n",
      "+----------+------------------+\n",
      "|     flood|13357.469178082192|\n",
      "|       dos| 311.8243034553761|\n",
      "| malformed| 20.68321195860483|\n",
      "|legitimate|7.7758887953888305|\n",
      "|   slowite| 3.937407365180709|\n",
      "|bruteforce| 3.916684747233673|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# average of length of TCP message grouped by target\n",
    "df_read.groupby('`target`').agg({'`tcp.len`': 'mean'}).orderBy('`avg(tcp.len)`', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd800c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X most frequent TCP flags\n",
    "\n",
    "def most_frequent(X):\n",
    "    tcp_flags_count_df = df_read.groupby('`tcp.flags`').count().orderBy('count', ascending = False)\n",
    "    flags_desc_list = tcp_flags_count_df.rdd.map(lambda x: x[0]).collect()\n",
    "    return flags_desc_list[0:X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34eaadde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slowite', 'bruteforce', 'flood', 'malformed', 'dos', 'legitimate']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most popular target on Twitter\n",
    "\n",
    "# get targets\n",
    "targets = df_read.select('`target`').distinct().rdd.map(lambda x: x[0]).collect()\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b948bc3-b47f-4a6c-9611-3528622ebbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b89bc21-1f7a-406e-87a1-12ca8475bd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zookeeper in /opt/conda/lib/python3.10/site-packages (1.3.3)\n",
      "Requirement already satisfied: tensorflow-datasets<4.7.0,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from zookeeper) (4.6.0)\n",
      "Requirement already satisfied: typeguard<2.14.0,>=2.5.1 in /opt/conda/lib/python3.10/site-packages (from zookeeper) (2.13.3)\n",
      "Requirement already satisfied: click<8.2.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from zookeeper) (8.1.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (1.23.4)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (3.19.6)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (1.11.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (1.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (2.28.1)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (0.10.2)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (2.1.1)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (2.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (4.64.1)\n",
      "Requirement already satisfied: etils[epath] in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (0.9.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (0.3.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (3.4)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[epath]->tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (3.10.0)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[epath]->tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (5.10.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from etils[epath]->tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (4.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets<4.7.0,>=1.3.0->zookeeper) (1.57.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%` not found.\n"
     ]
    }
   ],
   "source": [
    "!pip install zookeeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up KafkaConsumer to read tweets from Producer\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "topic_name = 'project-twitter-streaming'\n",
    "\n",
    "kafka_consumer = KafkaConsumer(\n",
    "    topic_name,\n",
    "    bootstrap_servers=['127.0.0.1:9092'],\n",
    "    auto_offset_reset='latest',\n",
    "    enable_auto_commit=True,\n",
    "    auto_commit_interval_ms =  5000,\n",
    "    fetch_max_bytes = 128,\n",
    "    max_poll_records = 100,\n",
    "    value_deserializer=lambda x: x.decode('utf-8'),\n",
    "    api_version=(0, 10, 1))\n",
    "\n",
    "# create dictionary to keep track of count of search terms\n",
    "counter_dict = dict(zip(targets, [0] * len(targets)))\n",
    "\n",
    "for message in kafka_consumer:\n",
    "    print(message.value)\n",
    "    \n",
    "    # split message\n",
    "    split = message.value.lower().split()\n",
    "    \n",
    "    # count frequencies\n",
    "    freq_df = spark.createDataFrame(split, \"string\").groupBy('value').count()\\\n",
    "    .orderBy('count', ascending = False)\n",
    "    \n",
    "    # extract count of search terms\n",
    "    freqs = freq_dff.filter(freq_dff.value.isin(targets)).rdd.map(lambda x: [x[0],x[1]]).collect()\n",
    "    \n",
    "    # update count dict\n",
    "    for freq in freqs:\n",
    "        counter_dict[freq[0]] += freq[1]\n",
    "    \n",
    "    print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddad0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after stream is over, plot frequencies\n",
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(list(counter_dict.keys()), list(counter_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dict in keys and vals\n",
    "keys = list(counter_dict.keys())\n",
    "vals = list(counter_dict.values())\n",
    "\n",
    "# find key with highest val\n",
    "print('Most popular attack on Twitter is:', keys[vals.index(max(vals))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f14ef72-878c-4cec-9e1f-b6f006bc04ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
