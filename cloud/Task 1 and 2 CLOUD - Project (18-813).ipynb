{"cells":[{"cell_type":"markdown","id":"639ce313","metadata":{},"source":["Variables to change:\n","\n","1. jar_file_loc: this is the file of the .jar postgres file. Change path to where it is located\n","2. postgres credentials (username, password, url): The default is linked to a PostgreSQL instance created in my GCP account. However, once I disable biling, it will be suspsended. I will be disabling billing to avoid wasting credits.\n","3. train_data_path & test_data_path: change to path of train/test csv files"]},{"cell_type":"code","execution_count":13,"id":"15f67f5a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["APP Name :GenericAppName\n","Master :local[*]\n"]}],"source":["jar_file_loc = 'gs://dataproc-staging-northamerica-northeast2-1049722977636-el8of1h1/postgresql-42.5.0.jar'\n","\n","# set up Spark\n","import pyspark\n","from pyspark import SparkContext, SparkConf, SQLContext\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","    .master(\"local[*]\") \\\n","    .appName(\"GenericAppName\") \\\n","    .config('spark.jars', jar_file_loc) \\\n","    .getOrCreate()\n","\n","\n","#Access SparkContext from your SparkSession\n","print(\"APP Name :\"+ spark.sparkContext.appName);\n","print(\"Master :\"+ spark.sparkContext.master);\n","\n","sqlContext = SQLContext(spark.sparkContext)\n"]},{"cell_type":"markdown","id":"a52c90b7","metadata":{},"source":["I tried using the augmented CSVs, but I couldn't succcessfully read it from Postgres after I wrote to it. The read command to more than 10 minutes and still wasn't able to produce a result. So, I decided to use the reduced dataset instead. I will run this on the cloud for the final submissions, so with the higher computation power, I should be able to analyze the augmented files."]},{"cell_type":"code","execution_count":14,"id":"462a6ec3","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# read in data\n","\n","train_data_path = 'gs://dataproc-staging-northamerica-northeast2-1049722977636-el8of1h1/train70_reduced.csv'\n","test_data_path = 'gs://dataproc-staging-northamerica-northeast2-1049722977636-el8of1h1/test30_reduced.csv'\n","\n","df_train = spark.read.csv(train_data_path, header = True, inferSchema = True)\n","df_test = spark.read.csv(test_data_path, header = True, inferSchema = True)"]},{"cell_type":"code","execution_count":15,"id":"95037873","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train 231646\n","Test 99290\n","Combined 330936\n"]}],"source":["# add column to differentiate b/w train and test sets\n","from pyspark.sql.functions import col, lit\n","\n","df_train_cat = df_train.withColumn(\"data_category\", lit(\"train\"))\n","df_test_cat = df_test.withColumn(\"data_category\", lit(\"test\"))\n","\n","print('Train', df_train_cat.count())\n","print('Test', df_test_cat.count())\n","\n","\n","# combine dfs\n","df_combined = df_train_cat.union(df_test_cat)\n","print('Combined', df_combined.count())"]},{"cell_type":"code","execution_count":16,"id":"1b68f8b9","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# write into postgresql db\n","\n","db_properties={}\n","#update your db username\n","db_properties['username']=\"postgres\"\n","#update your db password\n","db_properties['password']=\"bigdata\"\n","#make sure you got the right port number here\n","db_properties['url']= \"jdbc:postgresql://34.130.206.1/postgres\"\n","#make sure you had the Postgres JAR file in the right location\n","db_properties['driver']=\"org.postgresql.Driver\"\n","db_properties['table']= \"mqtt\"\n","\n","# create df with train data \n","df_combined.write.format(\"jdbc\")\\\n",".mode(\"overwrite\")\\\n",".option(\"url\", db_properties['url'])\\\n",".option(\"dbtable\", db_properties['table'])\\\n",".option(\"user\", db_properties['username'])\\\n",".option(\"password\", \"bigdata\")\\\n",".option(\"Driver\", db_properties['driver'])\\\n",".save()"]},{"cell_type":"code","execution_count":17,"id":"85f91954","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Item Count from PostgreSQL Read: 330936\n"]}],"source":["# read db to ensure data has been written in correctly\n","df_read = sqlContext.read.format(\"jdbc\")\\\n","    .option(\"url\", db_properties['url'])\\\n","    .option(\"dbtable\", db_properties['table'])\\\n","    .option(\"user\", db_properties['username'])\\\n","    .option(\"password\", \"bigdata\")\\\n","    .option(\"Driver\", db_properties['driver'])\\\n","    .load()\n","\n","print('Item Count from PostgreSQL Read:', df_read.count())"]},{"cell_type":"markdown","id":"787aa627","metadata":{},"source":["# Task II"]},{"cell_type":"code","execution_count":18,"id":"79fdee3c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Average length of a MQTT message in the training dataset is: 31.435725201384873\n"]}],"source":["# average length of a MQTT message in train\n","df_train = df_read.filter(df_read['data_category'] == 'train')\n","\n","print('Average length of a MQTT message in the training dataset is:',\\\n","      df_train.agg({'`mqtt.len`':'mean'}).collect()[0][0])"]},{"cell_type":"code","execution_count":19,"id":"35e25028","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+------------------+\n","|    target|      avg(tcp.len)|\n","+----------+------------------+\n","|     flood|13313.415986949429|\n","|       dos|312.65759830457716|\n","| malformed| 20.97491761259612|\n","|legitimate| 7.776101001432345|\n","|   slowite|3.9993479678330797|\n","|bruteforce|3.9871043376318873|\n","+----------+------------------+\n","\n"]}],"source":["# average of length of TCP message grouped by target\n","df_read.groupby('`target`').agg({'`tcp.len`': 'mean'}).orderBy('`avg(tcp.len)`', ascending = False).show()"]},{"cell_type":"code","execution_count":20,"id":"cd800c9a","metadata":{},"outputs":[{"data":{"text/plain":["['0x00000018', '0x00000010', '0x00000011']"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# X most frequent TCP flags\n","\n","def most_frequent(X):\n","    tcp_flags_count_df = df_read.groupby('`tcp.flags`').count().orderBy('count', ascending = False)\n","    flags_desc_list = tcp_flags_count_df.rdd.map(lambda x: x[0]).collect()\n","    return flags_desc_list[0:X]\n","\n","most_frequent(3)"]},{"cell_type":"code","execution_count":21,"id":"34eaadde","metadata":{},"outputs":[],"source":["# most popular target on Twitter\n","\n","# get targets\n","targets = df_read.select('`target`').distinct().rdd.map(lambda x: x[0]).collect()"]},{"cell_type":"markdown","id":"65e56d25","metadata":{},"source":["To get Kafka running, take these steps:\n","\n","1. Run ZooKeeper: zookeeper-server-start /opt/homebrew/etc/kafka/zookeeper.properties\n","2. Run Kafka: kafka-server-start /opt/homebrew/etc/kafka/server.properties\n","3. Create Kafka Topic: kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic project-twitter-streaming\n","4. In one Terminal shell, run: kafka-console-producer --broker-list localhost:9092 --topic project-twitter-streaming\n","5. In another Terminal shell, run: kafka-console-consumer --bootstrap-server localhost:9092 --topic project-twitter-streaming --from-beginning\n","6. Run the \"Twitter Streaming\" .ipynb notebook"]},{"cell_type":"code","execution_count":22,"id":"4982fc61","metadata":{},"outputs":[{"ename":"NoBrokersAvailable","evalue":"NoBrokersAvailable","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mNoBrokersAvailable\u001B[0m                        Traceback (most recent call last)","Cell \u001B[0;32mIn [22], line 10\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      8\u001B[0m topic_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproject-twitter-streaming\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 10\u001B[0m kafka_consumer \u001B[38;5;241m=\u001B[39m \u001B[43mKafkaConsumer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtopic_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbootstrap_servers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlocalhost:9092\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauto_offset_reset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlatest\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable_auto_commit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauto_commit_interval_ms\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m  \u001B[49m\u001B[38;5;241;43m5000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfetch_max_bytes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_poll_records\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalue_deserializer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# create dictionary to keep track of count of search terms\u001B[39;00m\n\u001B[1;32m     21\u001B[0m counter_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(targets, [\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(targets)))\n","File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/kafka/consumer/group.py:356\u001B[0m, in \u001B[0;36mKafkaConsumer.__init__\u001B[0;34m(self, *topics, **configs)\u001B[0m\n\u001B[1;32m    352\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mapi_version\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28mint\u001B[39m, str_version\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m)))\n\u001B[1;32m    353\u001B[0m     log\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse api_version=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m [tuple] -- \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m as str is deprecated\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    354\u001B[0m                 \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mapi_version\u001B[39m\u001B[38;5;124m'\u001B[39m]), str_version)\n\u001B[0;32m--> 356\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client \u001B[38;5;241m=\u001B[39m \u001B[43mKafkaClient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_metrics\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[38;5;66;03m# Get auto-discovered version from client if necessary\u001B[39;00m\n\u001B[1;32m    359\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mapi_version\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n","File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/kafka/client_async.py:244\u001B[0m, in \u001B[0;36mKafkaClient.__init__\u001B[0;34m(self, **configs)\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mapi_version\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    243\u001B[0m     check_timeout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mapi_version_auto_timeout_ms\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m1000\u001B[39m\n\u001B[0;32m--> 244\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mapi_version\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_version\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_timeout\u001B[49m\u001B[43m)\u001B[49m\n","File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/kafka/client_async.py:900\u001B[0m, in \u001B[0;36mKafkaClient.check_version\u001B[0;34m(self, node_id, timeout, strict)\u001B[0m\n\u001B[1;32m    898\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m try_node \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    899\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m--> 900\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Errors\u001B[38;5;241m.\u001B[39mNoBrokersAvailable()\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_connect(try_node)\n\u001B[1;32m    902\u001B[0m conn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conns[try_node]\n","\u001B[0;31mNoBrokersAvailable\u001B[0m: NoBrokersAvailable"]}],"source":["# set up KafkaConsumer to read tweets from Producer\n","from kafka import KafkaConsumer\n","import json\n","import time\n","import numpy as np\n","\n","\n","topic_name = 'project-twitter-streaming'\n","\n","kafka_consumer = KafkaConsumer(\n","    topic_name,\n","    bootstrap_servers=['localhost:9092'],\n","    auto_offset_reset='latest',\n","    enable_auto_commit=True,\n","    auto_commit_interval_ms =  5000,\n","    fetch_max_bytes = 128,\n","    max_poll_records = 100,\n","    value_deserializer=lambda x: x.decode('utf-8'))\n","\n","# create dictionary to keep track of count of search terms\n","counter_dict = dict(zip(targets, [0] * len(targets)))\n","\n","for message in kafka_consumer:\n","    print(message.value)\n","    \n","    # split message\n","    split = message.value.lower().split()\n","    \n","    # count frequencies\n","    freq_df = spark.createDataFrame(split, \"string\").groupBy('value').count()\\\n","    .orderBy('count', ascending = False)\n","    \n","    # extract count of search terms\n","    freqs = freq_dff.filter(freq_dff.value.isin(targets)).rdd.map(lambda x: [x[0],x[1]]).collect()\n","    \n","    # update count dict\n","    for freq in freqs:\n","        counter_dict[freq[0]] += freq[1]\n","    \n","    print(counter_dict)"]},{"cell_type":"code","execution_count":23,"id":"5ddad0c3","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'counter_dict' is not defined","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)","Cell \u001B[0;32mIn [23], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# after stream is over, plot frequencies\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m sns\u001B[38;5;241m.\u001B[39mbarplot(\u001B[38;5;28mlist\u001B[39m(\u001B[43mcounter_dict\u001B[49m\u001B[38;5;241m.\u001B[39mkeys()), \u001B[38;5;28mlist\u001B[39m(counter_dict\u001B[38;5;241m.\u001B[39mvalues()))\n","\u001B[0;31mNameError\u001B[0m: name 'counter_dict' is not defined"]}],"source":["# after stream is over, plot frequencies\n","import seaborn as sns\n","\n","sns.barplot(list(counter_dict.keys()), list(counter_dict.values()))"]},{"cell_type":"code","execution_count":null,"id":"a766d06d","metadata":{},"outputs":[],"source":["# split dict in keys and vals\n","keys = list(counter_dict.keys())\n","vals = list(counter_dict.values())\n","\n","# find key with highest val\n","print('Most popular attack on Twitter is:', keys[vals.index(max(vals))])"]},{"cell_type":"code","execution_count":null,"id":"9246347c","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}