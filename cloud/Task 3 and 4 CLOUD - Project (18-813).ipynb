{"cells":[{"cell_type":"markdown","metadata":{},"source":["Variables to change:\n","\n","1. jar_file_loc: this is the file of the .jar postgres file. Change path to where it is located\n","2. postgres credentials (username, password, url): The default is linked to a PostgreSQL instance created in my GCP account. However, once I disable biling, it will be suspsended. I will be disabling billing to avoid wasting credits.\n","3. train_data_path & test_data_path: change to path of train/test csv files"]},{"cell_type":"code","execution_count":2,"metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow==2.9.2\n","  Downloading tensorflow-2.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m511.8/511.8 MB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorflow==2.9.2) (1.16.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorflow==2.9.2) (4.4.0)\n","Collecting protobuf<3.20,>=3.9.2\n","  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m73.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hRequirement already satisfied: packaging in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorflow==2.9.2) (21.3)\n","Requirement already satisfied: setuptools in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorflow==2.9.2) (59.8.0)\n","Collecting google-pasta>=0.1.1\n","  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.5/57.5 kB\u001B[0m \u001B[31m12.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hCollecting gast<=0.4.0,>=0.2.1\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorflow==2.9.2) (1.14.1)\n","Collecting tensorboard<2.10,>=2.9\n","  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.8/5.8 MB\u001B[0m \u001B[31m94.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\n","\u001B[?25hCollecting h5py>=2.9.0\n","  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.5/4.5 MB\u001B[0m \u001B[31m107.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n","\u001B[?25hCollecting libclang>=13.0.0\n","  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m88.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n","  Downloading tensorflow_io_gcs_filesystem-0.28.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.4/2.4 MB\u001B[0m \u001B[31m101.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hCollecting keras<2.10.0,>=2.9.0rc0\n","  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m92.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorflow==2.9.2) (1.50.0)\n","Collecting numpy>=1.20\n","  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m17.1/17.1 MB\u001B[0m \u001B[31m79.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hCollecting astunparse>=1.6.0\n","  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n","Collecting flatbuffers<2,>=1.12\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Collecting absl-py>=1.0.0\n","  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.6/124.6 kB\u001B[0m \u001B[31m23.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n","  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m438.7/438.7 kB\u001B[0m \u001B[31m52.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hCollecting keras-preprocessing>=1.1.1\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.6/42.6 kB\u001B[0m \u001B[31m9.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hCollecting opt-einsum>=2.3.2\n","  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m65.5/65.5 kB\u001B[0m \u001B[31m13.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hCollecting termcolor>=1.1.0\n","  Downloading termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow==2.9.2) (0.38.2)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2) (3.3.7)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Collecting werkzeug>=1.0.1\n","  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m232.7/232.7 kB\u001B[0m \u001B[31m35.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hCollecting tensorboard-plugin-wit>=1.6.0\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m781.3/781.3 kB\u001B[0m \u001B[31m75.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2) (1.35.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.9/4.9 MB\u001B[0m \u001B[31m111.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n","\u001B[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2) (2.25.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from packaging->tensorflow==2.9.2) (2.4.7)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (0.2.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (4.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/miniconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (5.0.0)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (2022.9.24)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (2.10)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (2.1.1)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (3.10.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2) (3.2.2)\n","Installing collected packages: tensorboard-plugin-wit, libclang, keras, flatbuffers, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, protobuf, numpy, google-pasta, gast, astunparse, absl-py, opt-einsum, keras-preprocessing, h5py, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 0.5.3\n","    Uninstalling google-auth-oauthlib-0.5.3:\n","      Successfully uninstalled google-auth-oauthlib-0.5.3\n","\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pointpats 2.2.0 requires opencv-contrib-python>=4.2.0, which is not installed.\n","scipy 1.6.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.23.5 which is incompatible.\n","pysal 2.4.0 requires urllib3>=1.26, but you have urllib3 1.25.11 which is incompatible.\u001B[0m\u001B[31m\n","\u001B[0mSuccessfully installed absl-py-1.3.0 astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 h5py-3.7.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.6 numpy-1.23.5 opt-einsum-3.3.0 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.2 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.28.0 termcolor-2.1.1 werkzeug-2.2.2\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mCollecting numpy==1.21.6\n","  Downloading numpy-1.21.6-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n","\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.7/15.7 MB\u001B[0m \u001B[31m71.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pointpats 2.2.0 requires opencv-contrib-python>=4.2.0, which is not installed.\n","pysal 2.4.0 requires urllib3>=1.26, but you have urllib3 1.25.11 which is incompatible.\u001B[0m\u001B[31m\n","\u001B[0mSuccessfully installed numpy-1.21.6\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["# install if necessary \n","# AFTER INSTALLATION: restart Kernel\n","\n","!pip install tensorflow==2.9.2\n","!pip install numpy==1.21.6"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-11-28 21:29:04.819349: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2022-11-28 21:29:04.819394: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"]},{"name":"stdout","output_type":"stream","text":["2.9.2\n"]}],"source":["import tensorflow as tf  # now import the tensorflow module\n","print(tf.__version__)  # make sure the version is 2.x"]},{"cell_type":"markdown","metadata":{},"source":["# Task I"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","22/11/28 21:30:57 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","22/11/28 21:30:57 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","22/11/28 21:30:57 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/11/28 21:30:57 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"]},{"name":"stdout","output_type":"stream","text":["APP Name :GenericAppName\n","Master :local[*]\n"]}],"source":["jar_file_loc = 'gs://dataproc-staging-northamerica-northeast2-1049722977636-el8of1h1/postgresql-42.5.0.jar'\n","\n","# set up Spark\n","import pyspark\n","from pyspark import SparkContext, SparkConf, SQLContext\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","    .master(\"local[*]\") \\\n","    .appName(\"GenericAppName\") \\\n","    .config('spark.jars', jar_file_loc) \\\n","    .getOrCreate()\n","\n","\n","#Access SparkContext from your SparkSession\n","print(\"APP Name :\"+ spark.sparkContext.appName);\n","print(\"Master :\"+ spark.sparkContext.master);\n","\n","sqlContext = SQLContext(spark.sparkContext)\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# read in data\n","\n","train_data_path = 'gs://dataproc-staging-northamerica-northeast2-1049722977636-el8of1h1/train70_reduced.csv'\n","test_data_path = 'gs://dataproc-staging-northamerica-northeast2-1049722977636-el8of1h1/test30_reduced.csv'\n","\n","df_train = spark.read.csv(train_data_path, header = True, inferSchema = True)\n","df_test = spark.read.csv(test_data_path, header = True, inferSchema = True)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Item Count\n","\n","Train: 231646\n","Test: 99290\n","Combined: 330936\n"]}],"source":["# add column to differentiate b/w train and test sets\n","from pyspark.sql.functions import col, lit\n","\n","df_train_cat = df_train.withColumn(\"data_category\", lit(\"train\"))\n","df_test_cat = df_test.withColumn(\"data_category\", lit(\"test\"))\n","\n","print('Item Count\\n')\n","print('Train:', df_train_cat.count())\n","print('Test:', df_test_cat.count())\n","\n","\n","# combine dfs\n","df_combined = df_train_cat.union(df_test_cat)\n","print('Combined:', df_combined.count())"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# write into postgresql db\n","\n","db_properties={}\n","#update your db username\n","db_properties['username']=\"postgres\"\n","#update your db password\n","db_properties['password']=\"bigdata\"\n","#make sure you got the right port number here\n","db_properties['url']= \"jdbc:postgresql://34.130.206.1/postgres\"\n","#make sure you had the Postgres JAR file in the right location\n","db_properties['driver']=\"org.postgresql.Driver\"\n","db_properties['table']= \"mqtt\"\n","\n","# create df with train data \n","df_combined.write.format(\"jdbc\")\\\n",".mode(\"overwrite\")\\\n",".option(\"url\", db_properties['url'])\\\n",".option(\"dbtable\", db_properties['table'])\\\n",".option(\"user\", db_properties['username'])\\\n",".option(\"password\", \"bigdata\")\\\n",".option(\"Driver\", db_properties['driver'])\\\n",".save()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Item Count from PostgreSQL Read: 330936\n"]}],"source":["# read db to ensure data has been written in correctly\n","df_read = sqlContext.read.format(\"jdbc\")\\\n","    .option(\"url\", db_properties['url'])\\\n","    .option(\"dbtable\", db_properties['table'])\\\n","    .option(\"user\", db_properties['username'])\\\n","    .option(\"password\", \"bigdata\")\\\n","    .option(\"Driver\", db_properties['driver'])\\\n","    .load()\n","\n","print('Item Count from PostgreSQL Read:', df_read.count())"]},{"cell_type":"markdown","metadata":{},"source":["# Task III"]},{"cell_type":"markdown","metadata":{},"source":["### Data Processing"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# replacing all . with _ in column names to avoid bugs with having . in col name\n","\n","col_names_underscore = ['tcp_flags','tcp_time_delta','tcp_len','mqtt_conack_flags','mqtt_conack_flags_reserved',\n","             'mqtt_conack_flags_sp','mqtt_conack_val','mqtt_conflag_cleansess','mqtt_conflag_passwd',\n","             'mqtt_conflag_qos','mqtt_conflag_reserved','mqtt_conflag_retain','mqtt_conflag_uname',\n","             'mqtt_conflag_willflag','mqtt_conflags','mqtt_dupflag','mqtt_hdrflags','mqtt_kalive',\n","             'mqtt_len','mqtt_msg','mqtt_msgid','mqtt_msgtype','mqtt_proto_len','mqtt_protoname',\n","             'mqtt_qos','mqtt_retain','mqtt_sub_qos','mqtt_suback_qos','mqtt_ver','mqtt_willmsg',\n","             'mqtt_willmsg_len','mqtt_willtopic','mqtt_willtopic_len','target','data_category']\n","\n","df_read_underscore = df_read.toDF(*col_names_underscore)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# remove all zero columns & absurd cols\n","\n","zero_cols = ['mqtt_conack_flags','mqtt_conack_flags_reserved','mqtt_conack_flags_sp','mqtt_conack_val',\n","            'mqtt_conflag_qos','mqtt_conflag_reserved','mqtt_conflag_retain','mqtt_conflag_willflag',\n","            'mqtt_retain','mqtt_sub_qos','mqtt_suback_qos','mqtt_willmsg',\n","             'mqtt_willmsg_len','mqtt_willtopic','mqtt_willtopic_len']\n","\n","absurd_cols = ['mqtt_msg', 'mqtt_msgid']\n","\n","\n","df_read_underscore_dropped = df_read_underscore.drop(*zero_cols+absurd_cols)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# split train and test\n","df_read_train = df_read_underscore_dropped.where(df_read_underscore_dropped['data_category'] == 'train')\n","df_read_test = df_read_underscore_dropped.where(df_read_underscore_dropped['data_category'] == 'test')\n","\n","\n","# drop 'data_category' col\n","df_read_train = df_read_train.drop('data_category')\n","df_read_test = df_read_test.drop('data_category')"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# PySpark\n","import pyspark\n","from pyspark.sql import SparkSession, SQLContext\n","from pyspark.ml import Pipeline,Transformer\n","from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n","from pyspark.ml.classification import LogisticRegression\n","\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","import numpy as np\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["col_names_underscore = ['tcp_flags','tcp_time_delta','tcp_len','mqtt_conack_flags','mqtt_conack_flags_reserved',\n","             'mqtt_conack_flags_sp','mqtt_conack_val','mqtt_conflag_cleansess','mqtt_conflag_passwd',\n","             'mqtt_conflag_qos','mqtt_conflag_reserved','mqtt_conflag_retain','mqtt_conflag_uname',\n","             'mqtt_conflag_willflag','mqtt_conflags','mqtt_dupflag','mqtt_hdrflags','mqtt_kalive',\n","             'mqtt_len','mqtt_msg','mqtt_msgid','mqtt_msgtype','mqtt_proto_len','mqtt_protoname',\n","             'mqtt_qos','mqtt_retain','mqtt_sub_qos','mqtt_suback_qos','mqtt_ver','mqtt_willmsg',\n","             'mqtt_willmsg_len','mqtt_willtopic','mqtt_willtopic_len','target'] # 'data_category' removed\n","\n","\n","nominal_cols = ['tcp_flags','mqtt_conflags','mqtt_hdrflags', 'mqtt_protoname']\n","\n","continuous_cols = ['tcp_time_delta', 'tcp_len','mqtt_kalive','mqtt_len',  'mqtt_msgtype',\n","                  'mqtt_proto_len','mqtt_qos', 'mqtt_ver']\n","\n","binary_cols = ['mqtt_conflag_cleansess', 'mqtt_conflag_passwd','mqtt_conflag_uname', 'mqtt_dupflag']\n","\n","\n","keys = ['slowite', 'bruteforce','flood', 'malformed', 'dos', 'legitimate']\n","vals = [0, 1, 2, 3, 4, 5]\n","\n","global label_dict\n","label_dict = dict(zip(keys, vals))\n","\n","# ===========================================================================\n","\n","class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n","    \n","    def __init__(self):\n","        super().__init__()\n","\n","    def _transform(self, dataset):\n","        label_to_multiclass = udf(lambda name: label_dict[name])\n","        output_df = dataset.withColumn('outcome', label_to_multiclass(col('target'))).drop('target')\n","        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n","        return output_df\n","\n","class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n","    def __init__(self):\n","        super().__init__()\n","\n","    def _transform(self, dataset):\n","        output_df = dataset\n","        for col_name in binary_cols + continuous_cols:\n","            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n","\n","        return output_df\n","    \n","class ColumnDropper(Transformer): # this transformer drops uannecessary columns\n","    def __init__(self, columns_to_drop = None):\n","        super().__init__()\n","        self.columns_to_drop=columns_to_drop\n","    def _transform(self, dataset):\n","        output_df = dataset\n","        for col_name in self.columns_to_drop:\n","            output_df = output_df.drop(col_name)\n","        return output_df\n","\n","def get_preprocess_pipeline():\n","    # Stage where columns are casted as appropriate types\n","    stage_typecaster = FeatureTypeCaster()\n","\n","    # Stage where nominal columns are transformed to index columns using StringIndexer\n","    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n","    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n","    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n","    \n","    # Stage where the index columns are further transformed using OneHotEncoder\n","    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n","    \n","    # Stage where all relevant features are assembled into a vector (and dropping a few)\n","    feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n","    corelated_cols_to_remove = ['mqtt_conflag_uname','mqtt_qos','mqtt_proto_len', 'mqtt_ver']\n","\n","    for col_name in corelated_cols_to_remove:\n","        feature_cols.remove(col_name)\n","    \n","    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n","\n","    # Stage where we scale the columns\n","    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n","    \n","\n","    # Stage for creating the outcome column representing whether there is attack \n","    stage_outcome = OutcomeCreater()\n","\n","    # Removing all unnecessary columns, only keeping the 'features' and 'outcome' columns\n","    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols+nominal_id_cols+\n","        nominal_onehot_cols+ binary_cols + continuous_cols + ['vectorized_features'])\n","     \n","        \n","    # fit with logistic regression\n","    lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome', maxIter=10)\n","    \n","    # Connect the columns into a pipeline\n","    pipeline = Pipeline(stages=[stage_typecaster,stage_nominal_indexer,stage_nominal_onehot_encoder,\n","        stage_vector_assembler,stage_scaler,stage_outcome,stage_column_dropper])\n","    return pipeline"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# fit and transform\n","preprocess_pipeline = get_preprocess_pipeline()\n","preprocess_pipeline_model = preprocess_pipeline.fit(df_read_train)\n","\n","train_transform = preprocess_pipeline_model.transform(df_read_train)\n","test_transform = preprocess_pipeline_model.transform(df_read_test)"]},{"cell_type":"markdown","metadata":{},"source":["### Machine Learning - PySpark"]},{"cell_type":"markdown","metadata":{},"source":["The classifiers chosen are: Logistic Regression & Random Forest."]},{"cell_type":"markdown","metadata":{},"source":["#### Standard Training"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/11/28 21:32:13 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n","22/11/28 21:32:13 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n","Traceback (most recent call last):                                              \n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n","    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n","    raise EOFError\n","EOFError\n","                                                                                \r"]}],"source":["# logistic regression\n","\n","lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome', maxIter=10)\n","lr_fit = lr.fit(train_transform)\n","\n","lr_preds_train = lr_fit.transform(train_transform)\n","lr_preds_test = lr_fit.transform(test_transform)\n","\n","# random forest\n","from pyspark.ml.classification import RandomForestClassifier\n","\n","rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'outcome')\n","rf_fit = rf.fit(train_transform)\n","\n","rf_preds_train = rf_fit.transform(train_transform)\n","rf_preds_test = rf_fit.transform(test_transform)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Logistic Regression\n","Train Accuracy : 0.8226690726366956\n","Test Accuracy : 0.8217443851344546\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["\n","Random Forest\n","Train Accuracy : 0.8468266233822298\n","Test Accuracy : 0.8588881055494008\n"]}],"source":["# accuracies\n","\n","# logistic regression\n","lr_accuracy_train = (lr_preds_train.filter(lr_preds_train.outcome == lr_preds_train.prediction)\n","    .count() / float(lr_preds_train.count()))\n","\n","lr_accuracy_test = (lr_preds_test.filter(lr_preds_test.outcome == lr_preds_test.prediction)\n","    .count() / float(lr_preds_test.count()))\n","\n","print('Logistic Regression')\n","print(\"Train Accuracy :\", lr_accuracy_train)\n","print(\"Test Accuracy :\", lr_accuracy_test)\n","\n","\n","# rf\n","rf_accuracy_train = (rf_preds_train.filter(rf_preds_train.outcome == rf_preds_train.prediction)\n","    .count() / float(rf_preds_train.count()))\n","\n","rf_accuracy_test = (rf_preds_test.filter(rf_preds_test.outcome == rf_preds_test.prediction)\n","    .count() / float(rf_preds_test.count()))\n","\n","print('\\nRandom Forest')\n","print(\"Train Accuracy :\", rf_accuracy_train)\n","print(\"Test Accuracy :\", rf_accuracy_test)"]},{"cell_type":"markdown","metadata":{},"source":["#### Hyperparameter Tuning"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# logistic regression\n","\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome')\n","\n","# Create ParamGrid for Cross Validation\n","lr_paramGrid = (ParamGridBuilder()\n","             .addGrid(lr.regParam, [0.0001, 1.0])\n","             .addGrid(lr.maxIter, [10, 50])\n","             .build())\n","\n","evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', \n","    labelCol='outcome', metricName='accuracy')\n","\n","lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=lr_paramGrid, \n","                    evaluator=evaluator, numFolds=3)\n","\n","lr_cv_fit_train = lr_cv.fit(train_transform)\n","lr_cv_preds_test = lr_cv_fit_train.transform(test_transform)\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):                                              \n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n","    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n","    raise EOFError\n","EOFError\n","                                                                                \r"]}],"source":["# random forest\n","\n","rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'outcome')\n","\n","rf_paramGrid = (ParamGridBuilder()\n","             .addGrid(rf.maxDepth, [10, 15])# maximum depth for each tree\n","             .addGrid(rf.numTrees,[30, 60])# number of trues\n","             .build())\n","\n","evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', \n","    labelCol='outcome', metricName='accuracy')\n","\n","rf_cv = CrossValidator(estimator=rf, estimatorParamMaps=rf_paramGrid, \n","                    evaluator=evaluator, numFolds=3)\n","\n","rf_cv_fit_train = rf_cv.fit(train_transform)\n","\n","rf_cv_preds_test = rf_cv_fit_train.transform(test_transform)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 1034:>                                                       (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["\n","Logistic Regression\n","Pre-CV: 0.8217443851344546\n","Post-CV: 0.8266391378789405\n","\n","Random Forest\n","Pre-CV: 0.8588881055494008\n","Post-CV: 0.8970188337194078\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# logistic regression\n","lr_accuracy_test_cv = (lr_cv_preds_test.filter(lr_cv_preds_test.outcome == lr_cv_preds_test.prediction)\n","    .count() / float(lr_cv_preds_test.count()))\n","\n","# random forest\n","rf_accuracy_test_cv = (rf_cv_preds_test.filter(rf_cv_preds_test.outcome == rf_cv_preds_test.prediction)\n","    .count() / float(rf_cv_preds_test.count()))\n","\n","print('\\nLogistic Regression')\n","print(\"Pre-CV:\", lr_accuracy_test)\n","print(\"Post-CV:\", lr_accuracy_test_cv)\n","\n","\n","print(\"\\nRandom Forest\")\n","print(\"Pre-CV:\", rf_accuracy_test)\n","print(\"Post-CV:\", rf_accuracy_test_cv)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Machine Learning - TensorFlow"]},{"cell_type":"markdown","metadata":{},"source":["The two classifiers chosen are: a shallow NN and a deep NN. The shallow NN only has 2 hidden layers, while the deep NN has 5 hidden layers."]},{"cell_type":"code","execution_count":18,"metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-11-28 21:40:55.998412: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2022-11-28 21:40:55.998451: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"]},{"name":"stdout","output_type":"stream","text":["2.9.2\n"]}],"source":["import tensorflow as tf  # now import the tensorflow module\n","print(tf.__version__)  # make sure the version is 2.x"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import numpy as np\n","from tensorflow import keras"]},{"cell_type":"code","execution_count":20,"metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-11-28 21:41:54.069861: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","2022-11-28 21:41:54.069912: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n","2022-11-28 21:41:54.069941: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (default-50gbram-newnew-m): /proc/driver/nvidia/version does not exist\n","2022-11-28 21:41:54.070332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","                                                                                \r"]}],"source":["# create tensors\n","\n","x_train = tf.constant(np.array(train_transform.toPandas()['features'].values.tolist()))\n","y_train = tf.constant(np.array(train_transform.toPandas()['outcome'].values.tolist()))\n","\n","x_test = tf.constant(np.array(test_transform.toPandas()['features'].values.tolist()))\n","y_test = tf.constant(np.array(test_transform.toPandas()['outcome'].values.tolist()))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","7239/7239 - 8s - loss: 0.5159 - sparse_categorical_accuracy: 0.7947 - 8s/epoch - 1ms/step\n","Epoch 2/10\n","7239/7239 - 7s - loss: 0.4390 - sparse_categorical_accuracy: 0.8128 - 7s/epoch - 1ms/step\n","Epoch 3/10\n","7239/7239 - 7s - loss: 0.4324 - sparse_categorical_accuracy: 0.8185 - 7s/epoch - 1ms/step\n","Epoch 4/10\n","7239/7239 - 7s - loss: 0.4292 - sparse_categorical_accuracy: 0.8195 - 7s/epoch - 999us/step\n","Epoch 5/10\n","7239/7239 - 7s - loss: 0.4274 - sparse_categorical_accuracy: 0.8210 - 7s/epoch - 999us/step\n","Epoch 6/10\n","7239/7239 - 7s - loss: 0.4263 - sparse_categorical_accuracy: 0.8222 - 7s/epoch - 1000us/step\n","Epoch 7/10\n","7239/7239 - 7s - loss: 0.4256 - sparse_categorical_accuracy: 0.8220 - 7s/epoch - 1ms/step\n","Epoch 8/10\n","7239/7239 - 7s - loss: 0.4249 - sparse_categorical_accuracy: 0.8226 - 7s/epoch - 1ms/step\n","Epoch 9/10\n","7239/7239 - 7s - loss: 0.4245 - sparse_categorical_accuracy: 0.8228 - 7s/epoch - 1ms/step\n","Epoch 10/10\n","7239/7239 - 7s - loss: 0.4240 - sparse_categorical_accuracy: 0.8235 - 7s/epoch - 1ms/step\n","3103/3103 - 3s - loss: 0.4327 - sparse_categorical_accuracy: 0.8206 - 3s/epoch - 946us/step\n"]},{"data":{"text/plain":["[0.43267548084259033, 0.820636510848999]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Shallow NN (2 hidden layers)\n","from tensorflow import keras\n","\n","model_shallow = keras.Sequential()\n","\n","model_shallow.add(keras.layers.Dense(10, activation = 'relu'))\n","model_shallow.add(keras.layers.Dense(10, activation = 'relu'))\n","\n","model_shallow.add(keras.layers.Dense(6))\n","\n","model_shallow.compile(optimizer = keras.optimizers.SGD(), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n","model_shallow.fit(x_train, y_train, epochs = 10, verbose = 2)\n","model_shallow.evaluate(x_test, y_test, verbose = 2)"]},{"cell_type":"code","execution_count":22,"metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","7239/7239 - 8s - loss: 0.5618 - sparse_categorical_accuracy: 0.7683 - 8s/epoch - 1ms/step\n","Epoch 2/10\n","7239/7239 - 8s - loss: 0.4388 - sparse_categorical_accuracy: 0.8202 - 8s/epoch - 1ms/step\n","Epoch 3/10\n","7239/7239 - 8s - loss: 0.4310 - sparse_categorical_accuracy: 0.8242 - 8s/epoch - 1ms/step\n","Epoch 4/10\n","7239/7239 - 8s - loss: 0.4292 - sparse_categorical_accuracy: 0.8243 - 8s/epoch - 1ms/step\n","Epoch 5/10\n","7239/7239 - 8s - loss: 0.4277 - sparse_categorical_accuracy: 0.8247 - 8s/epoch - 1ms/step\n","Epoch 6/10\n","7239/7239 - 8s - loss: 0.4286 - sparse_categorical_accuracy: 0.8239 - 8s/epoch - 1ms/step\n","Epoch 7/10\n","7239/7239 - 8s - loss: 0.4253 - sparse_categorical_accuracy: 0.8249 - 8s/epoch - 1ms/step\n","Epoch 8/10\n","7239/7239 - 8s - loss: 0.4261 - sparse_categorical_accuracy: 0.8241 - 8s/epoch - 1ms/step\n","Epoch 9/10\n","7239/7239 - 8s - loss: 0.4245 - sparse_categorical_accuracy: 0.8247 - 8s/epoch - 1ms/step\n","Epoch 10/10\n","7239/7239 - 8s - loss: 0.4239 - sparse_categorical_accuracy: 0.8248 - 8s/epoch - 1ms/step\n","3103/3103 - 3s - loss: 0.4352 - sparse_categorical_accuracy: 0.8197 - 3s/epoch - 977us/step\n"]},{"data":{"text/plain":["[0.43518850207328796, 0.8196696639060974]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["from tensorflow import keras\n","\n","model_deep = keras.Sequential()\n","\n","model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n","model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n","model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n","model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n","model_deep.add(keras.layers.Dense(10, activation = 'relu'))\n","\n","\n","model_deep.add(keras.layers.Dense(6))\n","\n","model_deep.compile(optimizer = keras.optimizers.SGD(), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n","model_deep.fit(x_train, y_train, epochs = 10, verbose = 2)\n","model_deep.evaluate(x_test, y_test, verbose = 2)"]},{"cell_type":"markdown","metadata":{},"source":["#### Hyperparameter Tuning"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# combine the x and y train tensors and shuffle\n","tf_train = tf.concat([x_train, tf.reshape(y_train, [-1, 1])], axis = 1)\n","tf_train_shuffle = tf.random.shuffle(tf_train)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def shallow_cross_val_activation_width(k, act_fun, width):\n","    \n","    cuts = np.linspace(0, tf_train_shuffle.shape[0]-1, k+1, dtype = int)\n","    \n","    model = keras.Sequential()\n","    model.add(keras.layers.Dense(width, activation = act_fun))\n","    model.add(keras.layers.Dense(width, activation = act_fun))\n","    model.add(keras.layers.Dense(6))\n","\n","    metric = []\n","\n","    for i in range(k):\n","        print('k = '+str(i+1)+'\\n')\n","        \n","        val = tf_train_shuffle[cuts[i]:cuts[i+1]]\n","            \n","        mask = np.ones(tf_train_shuffle.shape[0])\n","        mask[cuts[i]:cuts[i+1]] = 0\n","        \n","        train = tf.boolean_mask(tf_train_shuffle, mask)\n","    \n","        cur_x_train = train[:,:-1]\n","        cur_y_train = train[:,-1]\n","        \n","        cur_x_val = val[:,:-1]\n","        cur_y_val = val[:,-1]\n","        \n","        model.compile(optimizer = keras.optimizers.SGD(), \n","                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n","        \n","        fit_data = model.fit(cur_x_train, cur_y_train, epochs = 5, \n","                             verbose = 2,validation_data = (cur_x_val, cur_y_val))\n","        \n","        cur_auc = np.mean(fit_data.history['val_sparse_categorical_accuracy'])\n","        metric.append(cur_auc)\n","        \n","        if np.max(metric) == cur_auc:\n","            best_model = model\n","            print('\\nNew best model saved.')\n","        \n","        print('\\n')\n","        \n","    print(metric)\n","    return np.mean(metric), best_model.evaluate(x_test, y_test, verbose = 2), best_model\n","\n","\n","# val_acc, test_res, best_model_shallow = shallow_cross_val_activation_width(k = 4, act_fun = 'relu', width = 5)\n","\n","# print('\\n===========')\n","# print(\"Validation Accuracy:\", val_acc)\n","# print(\"Test Accuracy:\", test_res[1])"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def deep_cross_val_activation_width(k, act_fun, width):\n","    \n","    cuts = np.linspace(0, tf_train_shuffle.shape[0]-1, k+1, dtype = int)\n","    \n","    model = keras.Sequential()\n","    \n","    model.add(keras.layers.Dense(width, activation = act_fun))\n","    model.add(keras.layers.Dense(width, activation = act_fun))\n","    model.add(keras.layers.Dense(width, activation = act_fun))\n","    model.add(keras.layers.Dense(width, activation = act_fun))\n","    model.add(keras.layers.Dense(width, activation = act_fun))\n","    \n","    model.add(keras.layers.Dense(6))\n","\n","    metric = []\n","\n","    for i in range(k):\n","        print('k = '+str(i+1)+'\\n')\n","        \n","        val = tf_train_shuffle[cuts[i]:cuts[i+1]]\n","            \n","        mask = np.ones(tf_train_shuffle.shape[0])\n","        mask[cuts[i]:cuts[i+1]] = 0\n","        \n","        train = tf.boolean_mask(tf_train_shuffle, mask)\n","    \n","        cur_x_train = train[:,:-1]\n","        cur_y_train = train[:,-1]\n","        \n","        cur_x_val = val[:,:-1]\n","        cur_y_val = val[:,-1]\n","        \n","        model.compile(optimizer = keras.optimizers.SGD(), \n","                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n","        \n","        fit_data = model.fit(cur_x_train, cur_y_train, epochs = 5,\n","                             verbose = 2, validation_data = (cur_x_val, cur_y_val))\n","        \n","        cur_auc = np.mean(fit_data.history['val_sparse_categorical_accuracy'])\n","        metric.append(cur_auc)\n","        \n","                \n","        if np.max(metric) == cur_auc:\n","            best_model = model\n","            print('\\nNew best model saved.')\n","\n","        print('\\n')\n","        \n","    print(metric)\n","    return np.mean(metric), best_model.evaluate(x_train, y_train, verbose = 2), best_model\n","\n","\n","# val_acc, test_res, best_model_deep = deep_cross_val_activation_width(k = 4, act_fun = 'relu', width = 5)\n","\n","# print('\\n===========')\n","# print(\"Validation Accuracy:\", val_acc)\n","# print(\"Test Accuracy:\", test_res[1])"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Starting trial: run-relu width10\n","k = 1\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 0.5326 - sparse_categorical_accuracy: 0.7962 - val_loss: 0.4558 - val_sparse_categorical_accuracy: 0.8188 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 7s - loss: 0.4435 - sparse_categorical_accuracy: 0.8160 - val_loss: 0.4481 - val_sparse_categorical_accuracy: 0.8214 - 7s/epoch - 1ms/step\n","Epoch 3/5\n","4826/4826 - 7s - loss: 0.4345 - sparse_categorical_accuracy: 0.8194 - val_loss: 0.4478 - val_sparse_categorical_accuracy: 0.8227 - 7s/epoch - 1ms/step\n","Epoch 4/5\n","4826/4826 - 7s - loss: 0.4294 - sparse_categorical_accuracy: 0.8224 - val_loss: 0.4342 - val_sparse_categorical_accuracy: 0.8235 - 7s/epoch - 1ms/step\n","Epoch 5/5\n","4826/4826 - 7s - loss: 0.4256 - sparse_categorical_accuracy: 0.8236 - val_loss: 0.4333 - val_sparse_categorical_accuracy: 0.8204 - 7s/epoch - 1ms/step\n","\n","New best model saved.\n","\n","\n","k = 2\n","\n","Epoch 1/5\n","4826/4826 - 7s - loss: 0.4298 - sparse_categorical_accuracy: 0.8229 - val_loss: 0.4274 - val_sparse_categorical_accuracy: 0.8267 - 7s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 7s - loss: 0.4286 - sparse_categorical_accuracy: 0.8236 - val_loss: 0.4206 - val_sparse_categorical_accuracy: 0.8244 - 7s/epoch - 1ms/step\n","Epoch 3/5\n","4826/4826 - 7s - loss: 0.4274 - sparse_categorical_accuracy: 0.8241 - val_loss: 0.4211 - val_sparse_categorical_accuracy: 0.8267 - 7s/epoch - 1ms/step\n","Epoch 4/5\n","4826/4826 - 7s - loss: 0.4272 - sparse_categorical_accuracy: 0.8242 - val_loss: 0.4228 - val_sparse_categorical_accuracy: 0.8267 - 7s/epoch - 1ms/step\n","Epoch 5/5\n","4826/4826 - 7s - loss: 0.4269 - sparse_categorical_accuracy: 0.8245 - val_loss: 0.4233 - val_sparse_categorical_accuracy: 0.8267 - 7s/epoch - 1ms/step\n","\n","New best model saved.\n","\n","\n","k = 3\n","\n","Epoch 1/5\n","4826/4826 - 7s - loss: 0.4273 - sparse_categorical_accuracy: 0.8241 - val_loss: 0.4174 - val_sparse_categorical_accuracy: 0.8285 - 7s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 7s - loss: 0.4268 - sparse_categorical_accuracy: 0.8244 - val_loss: 0.4177 - val_sparse_categorical_accuracy: 0.8285 - 7s/epoch - 1ms/step\n","Epoch 3/5\n","4826/4826 - 7s - loss: 0.4266 - sparse_categorical_accuracy: 0.8242 - val_loss: 0.4249 - val_sparse_categorical_accuracy: 0.8279 - 7s/epoch - 1ms/step\n","Epoch 4/5\n","4826/4826 - 7s - loss: 0.4260 - sparse_categorical_accuracy: 0.8244 - val_loss: 0.4182 - val_sparse_categorical_accuracy: 0.8285 - 7s/epoch - 1ms/step\n","Epoch 5/5\n","4826/4826 - 7s - loss: 0.4260 - sparse_categorical_accuracy: 0.8244 - val_loss: 0.4186 - val_sparse_categorical_accuracy: 0.8285 - 7s/epoch - 1ms/step\n","\n","New best model saved.\n","\n","\n","[0.8213533520698547, 0.8262617230415344, 0.8283520221710206]\n","3103/3103 - 3s - loss: 0.4371 - sparse_categorical_accuracy: 0.8266 - 3s/epoch - 916us/step\n","New Best Model: run-relu width10\n","\n","--- Starting trial: run-relu width20\n","k = 1\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 0.4691 - sparse_categorical_accuracy: 0.8085 - val_loss: 0.4449 - val_sparse_categorical_accuracy: 0.8209 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 7s - loss: 0.4294 - sparse_categorical_accuracy: 0.8204 - val_loss: 0.4315 - val_sparse_categorical_accuracy: 0.8235 - 7s/epoch - 1ms/step\n","Epoch 3/5\n","4826/4826 - 7s - loss: 0.4230 - sparse_categorical_accuracy: 0.8257 - val_loss: 0.4301 - val_sparse_categorical_accuracy: 0.8222 - 7s/epoch - 1ms/step\n","Epoch 4/5\n","4826/4826 - 7s - loss: 0.4202 - sparse_categorical_accuracy: 0.8265 - val_loss: 0.4281 - val_sparse_categorical_accuracy: 0.8236 - 7s/epoch - 1ms/step\n","Epoch 5/5\n","4826/4826 - 7s - loss: 0.4189 - sparse_categorical_accuracy: 0.8272 - val_loss: 0.4354 - val_sparse_categorical_accuracy: 0.8222 - 7s/epoch - 1ms/step\n","\n","New best model saved.\n","\n","\n","k = 2\n","\n","Epoch 1/5\n","4826/4826 - 7s - loss: 0.4231 - sparse_categorical_accuracy: 0.8253 - val_loss: 0.4224 - val_sparse_categorical_accuracy: 0.8268 - 7s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 7s - loss: 0.4225 - sparse_categorical_accuracy: 0.8256 - val_loss: 0.4174 - val_sparse_categorical_accuracy: 0.8268 - 7s/epoch - 1ms/step\n","Epoch 3/5\n","4826/4826 - 7s - loss: 0.4222 - sparse_categorical_accuracy: 0.8257 - val_loss: 0.4161 - val_sparse_categorical_accuracy: 0.8265 - 7s/epoch - 1ms/step\n","Epoch 4/5\n","4826/4826 - 7s - loss: 0.4219 - sparse_categorical_accuracy: 0.8256 - val_loss: 0.4165 - val_sparse_categorical_accuracy: 0.8268 - 7s/epoch - 1ms/step\n","Epoch 5/5\n","4826/4826 - 7s - loss: 0.4217 - sparse_categorical_accuracy: 0.8254 - val_loss: 0.4166 - val_sparse_categorical_accuracy: 0.8268 - 7s/epoch - 1ms/step\n","\n","New best model saved.\n","\n","\n","k = 3\n","\n","Epoch 1/5\n","4826/4826 - 7s - loss: 0.4228 - sparse_categorical_accuracy: 0.8250 - val_loss: 0.4145 - val_sparse_categorical_accuracy: 0.8277 - 7s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 7s - loss: 0.4225 - sparse_categorical_accuracy: 0.8248 - val_loss: 0.4143 - val_sparse_categorical_accuracy: 0.8285 - 7s/epoch - 1ms/step\n","Epoch 3/5\n","4826/4826 - 7s - loss: 0.4224 - sparse_categorical_accuracy: 0.8250 - val_loss: 0.4152 - val_sparse_categorical_accuracy: 0.8285 - 7s/epoch - 1ms/step\n","Epoch 4/5\n","4826/4826 - 7s - loss: 0.4222 - sparse_categorical_accuracy: 0.8248 - val_loss: 0.4141 - val_sparse_categorical_accuracy: 0.8283 - 7s/epoch - 1ms/step\n","Epoch 5/5\n","4826/4826 - 7s - loss: 0.4221 - sparse_categorical_accuracy: 0.8250 - val_loss: 0.4143 - val_sparse_categorical_accuracy: 0.8277 - 7s/epoch - 1ms/step\n","\n","New best model saved.\n","\n","\n","[0.8224982142448425, 0.8267486929893494, 0.8281525611877442]\n","3103/3103 - 3s - loss: 0.4315 - sparse_categorical_accuracy: 0.8205 - 3s/epoch - 924us/step\n","\n","--- Starting trial: run-softmax width10\n","k = 1\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 1.1208 - sparse_categorical_accuracy: 0.4984 - val_loss: 1.0835 - val_sparse_categorical_accuracy: 0.4956 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 7s - loss: 1.0718 - sparse_categorical_accuracy: 0.5022 - val_loss: 1.0764 - val_sparse_categorical_accuracy: 0.4956 - 7s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 7s - loss: 1.0534 - sparse_categorical_accuracy: 0.5059 - val_loss: 1.0007 - val_sparse_categorical_accuracy: 0.6077 - 7s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 7s - loss: 0.7984 - sparse_categorical_accuracy: 0.7766 - val_loss: 0.6975 - val_sparse_categorical_accuracy: 0.7798 - 7s/epoch - 1ms/step\n","Epoch 5/5\n","4826/4826 - 7s - loss: 0.6722 - sparse_categorical_accuracy: 0.7702 - val_loss: 0.6563 - val_sparse_categorical_accuracy: 0.7798 - 7s/epoch - 1ms/step\n","\n","New best model saved.\n","\n","\n","k = 2\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 0.6462 - sparse_categorical_accuracy: 0.7753 - val_loss: 0.6197 - val_sparse_categorical_accuracy: 0.7829 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 7s - loss: 0.6091 - sparse_categorical_accuracy: 0.7825 - val_loss: 0.5934 - val_sparse_categorical_accuracy: 0.7829 - 7s/epoch - 1ms/step\n","Epoch 3/5\n","4826/4826 - 7s - loss: 0.5882 - sparse_categorical_accuracy: 0.7825 - val_loss: 0.5808 - val_sparse_categorical_accuracy: 0.7829 - 7s/epoch - 1ms/step\n","Epoch 4/5\n","4826/4826 - 7s - loss: 0.5790 - sparse_categorical_accuracy: 0.7825 - val_loss: 0.5736 - val_sparse_categorical_accuracy: 0.7829 - 7s/epoch - 1ms/step\n","Epoch 5/5\n","4826/4826 - 7s - loss: 0.5739 - sparse_categorical_accuracy: 0.7825 - val_loss: 0.5696 - val_sparse_categorical_accuracy: 0.7829 - 7s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","k = 3\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 0.5726 - sparse_categorical_accuracy: 0.7814 - val_loss: 0.5628 - val_sparse_categorical_accuracy: 0.7852 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 7s - loss: 0.5701 - sparse_categorical_accuracy: 0.7813 - val_loss: 0.5606 - val_sparse_categorical_accuracy: 0.7852 - 7s/epoch - 1ms/step\n","Epoch 3/5\n","4826/4826 - 7s - loss: 0.5678 - sparse_categorical_accuracy: 0.7814 - val_loss: 0.5583 - val_sparse_categorical_accuracy: 0.7852 - 7s/epoch - 1ms/step\n","Epoch 4/5\n","4826/4826 - 7s - loss: 0.5655 - sparse_categorical_accuracy: 0.7814 - val_loss: 0.5579 - val_sparse_categorical_accuracy: 0.7852 - 7s/epoch - 1ms/step\n","Epoch 5/5\n","4826/4826 - 7s - loss: 0.5625 - sparse_categorical_accuracy: 0.7813 - val_loss: 0.5532 - val_sparse_categorical_accuracy: 0.7852 - 7s/epoch - 1ms/step\n","\n","New best model saved.\n","\n","\n","[0.6316881299018859, 0.782938551902771, 0.7852360010147095]\n","3103/3103 - 3s - loss: 0.5620 - sparse_categorical_accuracy: 0.7810 - 3s/epoch - 916us/step\n","\n","--- Starting trial: run-softmax width20\n","k = 1\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 1.1340 - sparse_categorical_accuracy: 0.4984 - val_loss: 1.0852 - val_sparse_categorical_accuracy: 0.4956 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 7s - loss: 1.0684 - sparse_categorical_accuracy: 0.5022 - val_loss: 1.0641 - val_sparse_categorical_accuracy: 0.4956 - 7s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 7s - loss: 0.9764 - sparse_categorical_accuracy: 0.6402 - val_loss: 0.8171 - val_sparse_categorical_accuracy: 0.7756 - 7s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 7s - loss: 0.6956 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.6410 - val_sparse_categorical_accuracy: 0.7756 - 7s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 7s - loss: 0.6106 - sparse_categorical_accuracy: 0.7822 - val_loss: 0.6056 - val_sparse_categorical_accuracy: 0.7796 - 7s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","k = 2\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 0.5921 - sparse_categorical_accuracy: 0.7824 - val_loss: 0.5830 - val_sparse_categorical_accuracy: 0.7829 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 7s - loss: 0.5825 - sparse_categorical_accuracy: 0.7824 - val_loss: 0.5761 - val_sparse_categorical_accuracy: 0.7829 - 7s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 7s - loss: 0.5779 - sparse_categorical_accuracy: 0.7824 - val_loss: 0.5732 - val_sparse_categorical_accuracy: 0.7827 - 7s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 7s - loss: 0.5751 - sparse_categorical_accuracy: 0.7823 - val_loss: 0.5694 - val_sparse_categorical_accuracy: 0.7828 - 7s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 7s - loss: 0.5727 - sparse_categorical_accuracy: 0.7824 - val_loss: 0.5687 - val_sparse_categorical_accuracy: 0.7828 - 7s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","k = 3\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 0.5700 - sparse_categorical_accuracy: 0.7812 - val_loss: 0.5598 - val_sparse_categorical_accuracy: 0.7852 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 7s - loss: 0.5660 - sparse_categorical_accuracy: 0.7812 - val_loss: 0.5553 - val_sparse_categorical_accuracy: 0.7852 - 7s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 7s - loss: 0.5621 - sparse_categorical_accuracy: 0.7813 - val_loss: 0.5522 - val_sparse_categorical_accuracy: 0.7852 - 7s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 7s - loss: 0.5589 - sparse_categorical_accuracy: 0.7812 - val_loss: 0.5508 - val_sparse_categorical_accuracy: 0.7852 - 7s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 7s - loss: 0.5553 - sparse_categorical_accuracy: 0.7813 - val_loss: 0.5496 - val_sparse_categorical_accuracy: 0.7852 - 7s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","[0.6643864512443542, 0.7828064441680909, 0.78517644405365]\n","3103/3103 - 3s - loss: 0.5582 - sparse_categorical_accuracy: 0.7810 - 3s/epoch - 919us/step\n","3103/3103 - 3s - loss: 0.4371 - sparse_categorical_accuracy: 0.8266 - 3s/epoch - 913us/step\n","3103/3103 - 3s - loss: 0.4371 - sparse_categorical_accuracy: 0.8266 - 3s/epoch - 907us/step\n","Best Model Hyperparameters: run-relu width10\n","Test Accuracy: 0.8265787363052368\n"]}],"source":["act_funs = ['relu', 'softmax']\n","widths = [10, 20]\n","\n","accuracies = []\n","\n","for act_fun in act_funs:\n","    for width in widths:\n","\n","        run_name = 'run-'+act_fun+' width'+str(width)\n","        print('')\n","        print('--- Starting trial: %s' % run_name)\n","\n","        run_dir = 'logs14813/hparam_tuning_q2_2/' + run_name\n","        val_acc, test_res, hp_model = shallow_cross_val_activation_width(3, act_fun, width)\n","        \n","        accuracies.append(test_res[1])\n","        \n","        if np.max(accuracies) == test_res[1]:\n","            best_hp_shallow_model = hp_model\n","            best_shallow_run_name = run_name\n","            print('New Best Model:', best_shallow_run_name)\n","            \n","best_hp_shallow_model.evaluate(x_test, y_test, verbose = 2)\n","shallow_res = best_hp_shallow_model.evaluate(x_test, y_test, verbose = 2)\n","\n","print('Best Model Hyperparameters:', best_shallow_run_name)\n","print('Test Accuracy:', shallow_res[1])"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Starting trial: run-relu width10\n","k = 1\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 0.5444 - sparse_categorical_accuracy: 0.7887 - val_loss: 0.4542 - val_sparse_categorical_accuracy: 0.8154 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 8s - loss: 0.4361 - sparse_categorical_accuracy: 0.8226 - val_loss: 0.4410 - val_sparse_categorical_accuracy: 0.8227 - 8s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 8s - loss: 0.4265 - sparse_categorical_accuracy: 0.8253 - val_loss: 0.4345 - val_sparse_categorical_accuracy: 0.8214 - 8s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 8s - loss: 0.4237 - sparse_categorical_accuracy: 0.8257 - val_loss: 0.4389 - val_sparse_categorical_accuracy: 0.8227 - 8s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 8s - loss: 0.4224 - sparse_categorical_accuracy: 0.8256 - val_loss: 0.4357 - val_sparse_categorical_accuracy: 0.8214 - 8s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","k = 2\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 0.4268 - sparse_categorical_accuracy: 0.8241 - val_loss: 0.4208 - val_sparse_categorical_accuracy: 0.8258 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 8s - loss: 0.4262 - sparse_categorical_accuracy: 0.8243 - val_loss: 0.4204 - val_sparse_categorical_accuracy: 0.8258 - 8s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 8s - loss: 0.4254 - sparse_categorical_accuracy: 0.8244 - val_loss: 0.4210 - val_sparse_categorical_accuracy: 0.8256 - 8s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 8s - loss: 0.4253 - sparse_categorical_accuracy: 0.8244 - val_loss: 0.4199 - val_sparse_categorical_accuracy: 0.8254 - 8s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 8s - loss: 0.4250 - sparse_categorical_accuracy: 0.8246 - val_loss: 0.4247 - val_sparse_categorical_accuracy: 0.8254 - 8s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","k = 3\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 0.4259 - sparse_categorical_accuracy: 0.8239 - val_loss: 0.4197 - val_sparse_categorical_accuracy: 0.8269 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 8s - loss: 0.4256 - sparse_categorical_accuracy: 0.8237 - val_loss: 0.4180 - val_sparse_categorical_accuracy: 0.8271 - 8s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 8s - loss: 0.4253 - sparse_categorical_accuracy: 0.8238 - val_loss: 0.4206 - val_sparse_categorical_accuracy: 0.8274 - 8s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 8s - loss: 0.4251 - sparse_categorical_accuracy: 0.8239 - val_loss: 0.4180 - val_sparse_categorical_accuracy: 0.8271 - 8s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 8s - loss: 0.4249 - sparse_categorical_accuracy: 0.8237 - val_loss: 0.4161 - val_sparse_categorical_accuracy: 0.8269 - 8s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","[0.8207446813583374, 0.8256064295768738, 0.827090585231781]\n","7239/7239 - 7s - loss: 0.4213 - sparse_categorical_accuracy: 0.8246 - 7s/epoch - 948us/step\n","New Best Model: run-relu width10\n","\n","--- Starting trial: run-relu width20\n","k = 1\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 0.4865 - sparse_categorical_accuracy: 0.8066 - val_loss: 0.4464 - val_sparse_categorical_accuracy: 0.8201 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 8s - loss: 0.4339 - sparse_categorical_accuracy: 0.8214 - val_loss: 0.4391 - val_sparse_categorical_accuracy: 0.8176 - 8s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 8s - loss: 0.4278 - sparse_categorical_accuracy: 0.8234 - val_loss: 0.4362 - val_sparse_categorical_accuracy: 0.8227 - 8s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 8s - loss: 0.4232 - sparse_categorical_accuracy: 0.8259 - val_loss: 0.4365 - val_sparse_categorical_accuracy: 0.8194 - 8s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 8s - loss: 0.4211 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.4394 - val_sparse_categorical_accuracy: 0.8219 - 8s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","k = 2\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 0.4250 - sparse_categorical_accuracy: 0.8246 - val_loss: 0.4204 - val_sparse_categorical_accuracy: 0.8256 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 8s - loss: 0.4244 - sparse_categorical_accuracy: 0.8245 - val_loss: 0.4208 - val_sparse_categorical_accuracy: 0.8258 - 8s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 8s - loss: 0.4241 - sparse_categorical_accuracy: 0.8247 - val_loss: 0.4193 - val_sparse_categorical_accuracy: 0.8258 - 8s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 8s - loss: 0.4235 - sparse_categorical_accuracy: 0.8250 - val_loss: 0.4194 - val_sparse_categorical_accuracy: 0.8254 - 8s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 8s - loss: 0.4233 - sparse_categorical_accuracy: 0.8252 - val_loss: 0.4223 - val_sparse_categorical_accuracy: 0.8264 - 8s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","k = 3\n","\n","Epoch 1/5\n","4826/4826 - 8s - loss: 0.4244 - sparse_categorical_accuracy: 0.8244 - val_loss: 0.4152 - val_sparse_categorical_accuracy: 0.8284 - 8s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 8s - loss: 0.4240 - sparse_categorical_accuracy: 0.8246 - val_loss: 0.4178 - val_sparse_categorical_accuracy: 0.8284 - 8s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 8s - loss: 0.4238 - sparse_categorical_accuracy: 0.8249 - val_loss: 0.4155 - val_sparse_categorical_accuracy: 0.8285 - 8s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 8s - loss: 0.4235 - sparse_categorical_accuracy: 0.8245 - val_loss: 0.4174 - val_sparse_categorical_accuracy: 0.8280 - 8s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 8s - loss: 0.4236 - sparse_categorical_accuracy: 0.8248 - val_loss: 0.4148 - val_sparse_categorical_accuracy: 0.8285 - 8s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","[0.820340609550476, 0.8258239984512329, 0.8283520102500915]\n","7239/7239 - 7s - loss: 0.4198 - sparse_categorical_accuracy: 0.8263 - 7s/epoch - 944us/step\n","New Best Model: run-relu width20\n","\n","--- Starting trial: run-softmax width10\n","k = 1\n","\n","Epoch 1/5\n","4826/4826 - 9s - loss: 1.1262 - sparse_categorical_accuracy: 0.5006 - val_loss: 1.0869 - val_sparse_categorical_accuracy: 0.4956 - 9s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 8s - loss: 1.0766 - sparse_categorical_accuracy: 0.5022 - val_loss: 1.0830 - val_sparse_categorical_accuracy: 0.4956 - 8s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 8s - loss: 1.0742 - sparse_categorical_accuracy: 0.5022 - val_loss: 1.0815 - val_sparse_categorical_accuracy: 0.4956 - 8s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 8s - loss: 1.0733 - sparse_categorical_accuracy: 0.5022 - val_loss: 1.0809 - val_sparse_categorical_accuracy: 0.4956 - 8s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 8s - loss: 1.0728 - sparse_categorical_accuracy: 0.5022 - val_loss: 1.0806 - val_sparse_categorical_accuracy: 0.4956 - 8s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","k = 2\n","\n","Epoch 1/5\n","4826/4826 - 9s - loss: 1.0760 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.0732 - val_sparse_categorical_accuracy: 0.5026 - 9s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 8s - loss: 1.0758 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.0731 - val_sparse_categorical_accuracy: 0.5026 - 8s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 8s - loss: 1.0757 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.0730 - val_sparse_categorical_accuracy: 0.5026 - 8s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 8s - loss: 1.0756 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.0729 - val_sparse_categorical_accuracy: 0.5026 - 8s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 8s - loss: 1.0755 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.0729 - val_sparse_categorical_accuracy: 0.5026 - 8s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","k = 3\n","\n","Epoch 1/5\n","4826/4826 - 9s - loss: 1.0764 - sparse_categorical_accuracy: 0.4991 - val_loss: 1.0710 - val_sparse_categorical_accuracy: 0.5019 - 9s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 8s - loss: 1.0764 - sparse_categorical_accuracy: 0.4991 - val_loss: 1.0710 - val_sparse_categorical_accuracy: 0.5019 - 8s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 8s - loss: 1.0764 - sparse_categorical_accuracy: 0.4991 - val_loss: 1.0710 - val_sparse_categorical_accuracy: 0.5019 - 8s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 8s - loss: 1.0764 - sparse_categorical_accuracy: 0.4991 - val_loss: 1.0711 - val_sparse_categorical_accuracy: 0.5019 - 8s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 8s - loss: 1.0764 - sparse_categorical_accuracy: 0.4991 - val_loss: 1.0709 - val_sparse_categorical_accuracy: 0.5019 - 8s/epoch - 2ms/step\n","\n","\n","[0.495564341545105, 0.5025836825370789, 0.5018584728240967]\n","7239/7239 - 7s - loss: 1.0745 - sparse_categorical_accuracy: 0.5000 - 7s/epoch - 954us/step\n","\n","--- Starting trial: run-softmax width20\n","k = 1\n","\n","Epoch 1/5\n","4826/4826 - 9s - loss: 1.1350 - sparse_categorical_accuracy: 0.4996 - val_loss: 1.0877 - val_sparse_categorical_accuracy: 0.4956 - 9s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 8s - loss: 1.0771 - sparse_categorical_accuracy: 0.5022 - val_loss: 1.0833 - val_sparse_categorical_accuracy: 0.4956 - 8s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 8s - loss: 1.0745 - sparse_categorical_accuracy: 0.5022 - val_loss: 1.0817 - val_sparse_categorical_accuracy: 0.4956 - 8s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 8s - loss: 1.0734 - sparse_categorical_accuracy: 0.5022 - val_loss: 1.0812 - val_sparse_categorical_accuracy: 0.4956 - 8s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 8s - loss: 1.0728 - sparse_categorical_accuracy: 0.5022 - val_loss: 1.0810 - val_sparse_categorical_accuracy: 0.4956 - 8s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","k = 2\n","\n","Epoch 1/5\n","4826/4826 - 9s - loss: 1.0761 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.0733 - val_sparse_categorical_accuracy: 0.5026 - 9s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 8s - loss: 1.0759 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.0731 - val_sparse_categorical_accuracy: 0.5026 - 8s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 8s - loss: 1.0757 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.0731 - val_sparse_categorical_accuracy: 0.5026 - 8s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 8s - loss: 1.0756 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.0731 - val_sparse_categorical_accuracy: 0.5026 - 8s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 8s - loss: 1.0756 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.0732 - val_sparse_categorical_accuracy: 0.5026 - 8s/epoch - 2ms/step\n","\n","New best model saved.\n","\n","\n","k = 3\n","\n","Epoch 1/5\n","4826/4826 - 9s - loss: 1.0765 - sparse_categorical_accuracy: 0.4991 - val_loss: 1.0711 - val_sparse_categorical_accuracy: 0.5019 - 9s/epoch - 2ms/step\n","Epoch 2/5\n","4826/4826 - 8s - loss: 1.0764 - sparse_categorical_accuracy: 0.4991 - val_loss: 1.0711 - val_sparse_categorical_accuracy: 0.5019 - 8s/epoch - 2ms/step\n","Epoch 3/5\n","4826/4826 - 8s - loss: 1.0764 - sparse_categorical_accuracy: 0.4991 - val_loss: 1.0710 - val_sparse_categorical_accuracy: 0.5019 - 8s/epoch - 2ms/step\n","Epoch 4/5\n","4826/4826 - 8s - loss: 1.0764 - sparse_categorical_accuracy: 0.4991 - val_loss: 1.0710 - val_sparse_categorical_accuracy: 0.5019 - 8s/epoch - 2ms/step\n","Epoch 5/5\n","4826/4826 - 8s - loss: 1.0764 - sparse_categorical_accuracy: 0.4991 - val_loss: 1.0710 - val_sparse_categorical_accuracy: 0.5019 - 8s/epoch - 2ms/step\n","\n","\n","[0.495564341545105, 0.5025836825370789, 0.5018584728240967]\n","7239/7239 - 7s - loss: 1.0746 - sparse_categorical_accuracy: 0.5000 - 7s/epoch - 972us/step\n","3103/3103 - 3s - loss: 0.4286 - sparse_categorical_accuracy: 0.8267 - 3s/epoch - 949us/step\n","Best Model Hyperparameters: run-relu width20\n","Test Accuracy: 0.8266894817352295\n"]}],"source":["act_funs = ['relu', 'softmax']\n","widths = [10, 20]\n","\n","accuracies = []\n","\n","for act_fun in act_funs:\n","    for width in widths:\n","\n","        run_name = 'run-'+act_fun+' width'+str(width)\n","        print('')\n","        print('--- Starting trial: %s' % run_name)\n","        \n","        val_acc, test_res, hp_model = deep_cross_val_activation_width(3, act_fun, width)\n","        \n","        accuracies.append(test_res[1])\n","        \n","        if np.max(accuracies) == test_res[1]:\n","            best_hp_deep_model = hp_model\n","            best_deep_run_name = run_name\n","            print('New Best Model:', best_deep_run_name)\n","\n","\n","deep_res = best_hp_deep_model.evaluate(x_test, y_test, verbose = 2)\n","\n","print('Best Model Hyperparameters:', best_deep_run_name)\n","print('Test Accuracy:', deep_res[1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":2}